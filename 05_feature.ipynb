{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da944f99-542c-4862-88e8-f4806bc7d7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold,GroupKFold\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import re\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import json\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8eb07835-818c-4228-9e4e-76bd4adb3ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec_feature(seq,emb,feat,ikx,ext='',prex = '',feature=[]):\n",
    "    sentence = [[str(x) for x in x] for x in seq]\n",
    "    if os.path.exists('w2v/w2v_model_{}_{}_{}.model'.format(prex,'_'.join(feat),ext)):\n",
    "        model = Word2Vec.load('w2v/w2v_model_{}_{}_{}.model'.format(prex,'_'.join(feat),ext))\n",
    "    else:\n",
    "        model = Word2Vec(sentence, vector_size=emb, window=5, min_count=1, workers=8, epochs=10, sg=1, seed=42)\n",
    "        model.save('w2v/w2v_model_{}_{}_{}.model'.format(prex,'_'.join(feat),ext))\n",
    "    return model\n",
    "\n",
    "def generate_w2v_feat(df,prex,col_name,dim):\n",
    "\n",
    "    feature = []\n",
    "    f = col_name\n",
    "    dim = dim\n",
    "\n",
    "    \n",
    "    res = df.groupby(prex)[f].apply(lambda x:list(x)).reset_index()\n",
    "    model = get_word2vec_feature(res[f].values,dim,[prex,f],f,ext='{}'.format(dim),prex = prex,feature=[])\n",
    "\n",
    "    i = 0\n",
    "    emb_matrix = []\n",
    "    for col in tqdm(res[f].values):\n",
    "        tmp = [model.wv[str(seq)] for seq in col]\n",
    "        tmp = np.mean(tmp,axis = 0)\n",
    "        emb_matrix.append(tmp)\n",
    "    emb_matrix = np.array(emb_matrix)\n",
    "\n",
    "    for i in range(dim):\n",
    "        res['{}_{}_{}'.format(prex,f + '_emb_mean',i)] = emb_matrix[:,i]\n",
    "        feature.append('{}_{}_{}'.format(prex,f + '_emb_mean',i))\n",
    "\n",
    "\n",
    "\n",
    "    joblib.dump(res[[prex,col_name] +feature ],'feats/w2v_{}_{}_emb.pkl'.format(prex,f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2621243f-ef78-4be2-9936-1caf267155db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_char(input_values, output_num, output_prefix, seed=1024):\n",
    "    tfidf_enc = TfidfVectorizer(ngram_range=(1, 4), analyzer=\"char_wb\")\n",
    "    #tfidf_enc = TfidfVectorizer(ngram_range=(1, 2))\n",
    "    #tfidf_enc = TfidfVectorizer()\n",
    "\n",
    "    tfidf_vec = tfidf_enc.fit_transform(input_values)\n",
    "    svd_tmp = TruncatedSVD(n_components=output_num, n_iter=20, random_state=seed)\n",
    "    svd_tmp = svd_tmp.fit_transform(tfidf_vec)\n",
    "    svd_tmp = pd.DataFrame(svd_tmp)\n",
    "    svd_tmp.columns = ['{}_tfidf_char_{}'.format(output_prefix, i) for i in range(output_num)]\n",
    "    return svd_tmp\n",
    "\n",
    "def tfidf_word(input_values, output_num, output_prefix, seed=1024):\n",
    "    tfidf_enc = TfidfVectorizer(ngram_range=(1, 4),sublinear_tf = True)\n",
    "    #tfidf_enc = TfidfVectorizer(ngram_range=(1, 2))\n",
    "    #tfidf_enc = TfidfVectorizer()\n",
    "\n",
    "    tfidf_vec = tfidf_enc.fit_transform(input_values)\n",
    "    svd_tmp = TruncatedSVD(n_components=output_num, n_iter=20, random_state=seed)\n",
    "    svd_tmp = svd_tmp.fit_transform(tfidf_vec)\n",
    "    svd_tmp = pd.DataFrame(svd_tmp)\n",
    "    svd_tmp.columns = ['{}_tfidf_word_{}'.format(output_prefix, i) for i in range(output_num)]\n",
    "    return svd_tmp\n",
    "\n",
    "def count2vec(input_values, output_num, output_prefix, seed=1024):\n",
    "    #count_enc = CountVectorizer(ngram_range=(1, 3), analyzer=\"char_wb\")\n",
    "    count_enc = CountVectorizer(ngram_range=(1, 4))\n",
    "\n",
    "    count_vec = count_enc.fit_transform(input_values)\n",
    "    svd_tmp = TruncatedSVD(n_components=output_num, n_iter=20, random_state=seed)\n",
    "    svd_tmp = svd_tmp.fit_transform(count_vec)\n",
    "    svd_tmp = pd.DataFrame(svd_tmp)\n",
    "    svd_tmp.columns = ['{}_countvec_{}'.format(output_prefix, i) for i in range(output_num)]\n",
    "    return svd_tmp\n",
    "\n",
    "\n",
    "def  get_tfidf(tmp,group_id, group_target, num):\n",
    "    #tmp[group_target] = tmp[group_target].apply(lambda x: ' '.join(x))\n",
    "    tfidf_tmp1 = tfidf_word(tmp[group_target], num, group_target)\n",
    "    #tfidf_tmp2 = tfidf_char(tmp[group_target], num, group_target)\n",
    "\n",
    "    count_tmp = count2vec(tmp[group_target], num, group_target)\n",
    "    return pd.concat([tmp[group_id], tfidf_tmp1,count_tmp], axis=1)\n",
    "    #return pd.concat([tmp[group_id], tfidf_tmp], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66509d81-fd35-40e2-9b36-549117eaf84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../IND-WhoIsWho/pid_to_info_all.json', 'r') as file:\n",
    "    pid = json.load(file)\n",
    "train = pd.read_feather('data/train.feather')\n",
    "valid = pd.read_feather('data/valid.feather')\n",
    "test = pd.read_feather('data/test.feather')\n",
    "\n",
    "piddf = joblib.load('data/pid_df.pkl')\n",
    "\n",
    "data = pd.concat([train,valid,test]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6c06770-30d5-47a4-a9f6-1f26202434c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_clean(x,mode = 1):\n",
    "    x = x.lower()\n",
    "    for f in list('-?=～—/_？）￥:#\\\\\\'.》”^>$]}|+)、（&{`《,(%!“<’\"】；【‘~*@…：，。[;') :\n",
    "        x = x.replace(f,'')\n",
    "        \n",
    "    if mode == 'venue':\n",
    "        number_pattern = r'\\d+'\n",
    "        x = re.sub(number_pattern, '', x)\n",
    "    \n",
    "    \n",
    "    for i in range(3):\n",
    "        x = x.replace('  ',' ')\n",
    "    return x\n",
    "piddf['title'] = piddf['title'].apply(text_clean)\n",
    "piddf['abstract'] = piddf['abstract'].apply(text_clean)\n",
    "\n",
    "piddf['venue'] = piddf['venue'].fillna('')\n",
    "piddf['venue'] = piddf['venue'].apply(lambda x:text_clean(x,mode = 'venue'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81833334-cf19-44c1-9dc2-a6659856fb92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>abstract</th>\n",
       "      <th>keywords</th>\n",
       "      <th>venue</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EgcXuw3e</td>\n",
       "      <td>fuzzy adaptive pid control of large erecting s...</td>\n",
       "      <td>[{'name': 'Liang Li', 'org': 'Xi'an Research I...</td>\n",
       "      <td>in considering nonlinearity and uncertainty in...</td>\n",
       "      <td>[Adaptive Control, Electro-Hydraulics, Erectin...</td>\n",
       "      <td>journal of theoretical and applied information...</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9NFbioNk</td>\n",
       "      <td>when is scene identification just texture reco...</td>\n",
       "      <td>[{'name': 'Laura Walker Renninger', 'org': 'Ey...</td>\n",
       "      <td>subjects were asked to identify scenes after v...</td>\n",
       "      <td>[categorization, computer vision]</td>\n",
       "      <td>vision research</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fai4LpQ3</td>\n",
       "      <td>a computational model for shape from texture</td>\n",
       "      <td>[{'name': 'J Malik', 'org': 'Department of Ele...</td>\n",
       "      <td>shape from texture is best analysed in a twost...</td>\n",
       "      <td>[image plane, shape]</td>\n",
       "      <td>ciba foundation symposium</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H5smGgSX</td>\n",
       "      <td>on the implicit assumptions of gans</td>\n",
       "      <td>[{'name': 'Ke Li', 'org': ''}, {'name': 'Jiten...</td>\n",
       "      <td>generative adversarial nets gans have generate...</td>\n",
       "      <td></td>\n",
       "      <td>arxiv learning</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8gAJkJa7</td>\n",
       "      <td>coupling visualization and data analysis for k...</td>\n",
       "      <td>[{'name': 'Oliver Rübel', 'org': ''}, {'name':...</td>\n",
       "      <td>knowledge discovery from large and complex sci...</td>\n",
       "      <td>[scientific data, scientific visualization, de...</td>\n",
       "      <td>iccs</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2952</th>\n",
       "      <td>40OWywB0</td>\n",
       "      <td>temporal variation of total gaseous mercury in...</td>\n",
       "      <td>[{'name': 'Xinbin Feng', 'org': 'State Key Lab...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>journal of geophysical research</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2953</th>\n",
       "      <td>HWscmrvG</td>\n",
       "      <td>total gaseous mercury emissions from mercuryen...</td>\n",
       "      <td>[{'name': 'xinbin', 'org': 'chinese academy of...</td>\n",
       "      <td></td>\n",
       "      <td>[flux, natural source, emission, soil, mercury...</td>\n",
       "      <td>chinese journal of geochemistry</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2954</th>\n",
       "      <td>fHochnD0</td>\n",
       "      <td>total gaseous mercury in the atmosphere of gui...</td>\n",
       "      <td>[{'name': 'Xinbin Feng', 'org': ''}, {'name': ...</td>\n",
       "      <td>four measurement campaigns were carried out to...</td>\n",
       "      <td>[source, atmosphere, guiyang, mercury measurem...</td>\n",
       "      <td>the science of the total environment</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2955</th>\n",
       "      <td>zELKcnoE</td>\n",
       "      <td>total gaseous mercury emissions from soil in g...</td>\n",
       "      <td>[{'name': 'Xinbin Feng', 'org': 'State Key Lab...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>journal of geophysical research</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2956</th>\n",
       "      <td>m9ibagxZ</td>\n",
       "      <td>title study on trace elements of water in xiao...</td>\n",
       "      <td>[{'name': 'Shunlin TANG', 'org': ''}, {'name':...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2957 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                              title  \\\n",
       "0     EgcXuw3e  fuzzy adaptive pid control of large erecting s...   \n",
       "1     9NFbioNk  when is scene identification just texture reco...   \n",
       "2     fai4LpQ3       a computational model for shape from texture   \n",
       "3     H5smGgSX                on the implicit assumptions of gans   \n",
       "4     8gAJkJa7  coupling visualization and data analysis for k...   \n",
       "...        ...                                                ...   \n",
       "2952  40OWywB0  temporal variation of total gaseous mercury in...   \n",
       "2953  HWscmrvG  total gaseous mercury emissions from mercuryen...   \n",
       "2954  fHochnD0  total gaseous mercury in the atmosphere of gui...   \n",
       "2955  zELKcnoE  total gaseous mercury emissions from soil in g...   \n",
       "2956  m9ibagxZ  title study on trace elements of water in xiao...   \n",
       "\n",
       "                                                authors  \\\n",
       "0     [{'name': 'Liang Li', 'org': 'Xi'an Research I...   \n",
       "1     [{'name': 'Laura Walker Renninger', 'org': 'Ey...   \n",
       "2     [{'name': 'J Malik', 'org': 'Department of Ele...   \n",
       "3     [{'name': 'Ke Li', 'org': ''}, {'name': 'Jiten...   \n",
       "4     [{'name': 'Oliver Rübel', 'org': ''}, {'name':...   \n",
       "...                                                 ...   \n",
       "2952  [{'name': 'Xinbin Feng', 'org': 'State Key Lab...   \n",
       "2953  [{'name': 'xinbin', 'org': 'chinese academy of...   \n",
       "2954  [{'name': 'Xinbin Feng', 'org': ''}, {'name': ...   \n",
       "2955  [{'name': 'Xinbin Feng', 'org': 'State Key Lab...   \n",
       "2956  [{'name': 'Shunlin TANG', 'org': ''}, {'name':...   \n",
       "\n",
       "                                               abstract  \\\n",
       "0     in considering nonlinearity and uncertainty in...   \n",
       "1     subjects were asked to identify scenes after v...   \n",
       "2     shape from texture is best analysed in a twost...   \n",
       "3     generative adversarial nets gans have generate...   \n",
       "4     knowledge discovery from large and complex sci...   \n",
       "...                                                 ...   \n",
       "2952                                                      \n",
       "2953                                                      \n",
       "2954  four measurement campaigns were carried out to...   \n",
       "2955                                                      \n",
       "2956                                                      \n",
       "\n",
       "                                               keywords  \\\n",
       "0     [Adaptive Control, Electro-Hydraulics, Erectin...   \n",
       "1                     [categorization, computer vision]   \n",
       "2                                  [image plane, shape]   \n",
       "3                                                         \n",
       "4     [scientific data, scientific visualization, de...   \n",
       "...                                                 ...   \n",
       "2952                                                      \n",
       "2953  [flux, natural source, emission, soil, mercury...   \n",
       "2954  [source, atmosphere, guiyang, mercury measurem...   \n",
       "2955                                                      \n",
       "2956                                                 []   \n",
       "\n",
       "                                                  venue  year  \n",
       "0     journal of theoretical and applied information...  2013  \n",
       "1                                       vision research  2004  \n",
       "2                             ciba foundation symposium  1994  \n",
       "3                                        arxiv learning  2018  \n",
       "4                                                  iccs  2010  \n",
       "...                                                 ...   ...  \n",
       "2952                    journal of geophysical research  2004  \n",
       "2953                    chinese journal of geochemistry  2006  \n",
       "2954               the science of the total environment  2003  \n",
       "2955                    journal of geophysical research  2005  \n",
       "2956                                                     2010  \n",
       "\n",
       "[2957 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "piddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63d4faf6-8a86-4775-8a4f-fbd2fe42f7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "piddf = piddf.reset_index(drop = True)\n",
    "piddf['title'] = piddf['title'].apply(lambda x:x.lower())\n",
    "piddf['index'] = piddf.index + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da19e98f-9d0d-41bb-a69e-4deb3b555b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec_feature(seq,emb,feat,ikx,ext='',prex = '',feature=[]):\n",
    "    sentence = [[str(x) for x in x] for x in seq]\n",
    "    if os.path.exists('w2v/w2v_model_{}_{}_{}.model'.format(prex,'_'.join(feat),ext)):\n",
    "        model = Word2Vec.load('w2v/w2v_model_{}_{}_{}.model'.format(prex,'_'.join(feat),ext))\n",
    "    else:\n",
    "        model = Word2Vec(sentence, vector_size=emb, window=5, min_count=1, workers=8, epochs=10, sg=1, seed=42)\n",
    "        model.save('w2v/w2v_model_{}_{}_{}.model'.format(prex,'_'.join(feat),ext))\n",
    "    return model\n",
    "\n",
    "def generate_w2v_feat(df,prex,col_name,dim):\n",
    "\n",
    "    feature = []\n",
    "    f = col_name\n",
    "    dim = dim\n",
    "    res = df.copy()\n",
    "    \n",
    "    #res[f] = df[f].apply(lambda x:x.split(' '))\n",
    "    model = get_word2vec_feature(res[f].values,dim,[prex,f],f,ext='{}'.format(dim),prex = prex,feature=[])\n",
    "\n",
    "    i = 0\n",
    "    emb_matrix = []\n",
    "    \n",
    "    for col in tqdm(res[f].values):\n",
    "        tmp = [model.wv[str(seq)] for seq in col]\n",
    "        tmp = np.mean(tmp,axis = 0)\n",
    "        emb_matrix.append(tmp)\n",
    "    emb_matrix = np.array(emb_matrix)\n",
    "\n",
    "    for i in range(dim):\n",
    "        res['{}_{}_{}'.format(prex,f + '_emb_mean',i)] = emb_matrix[:,i]\n",
    "        feature.append('{}_{}_{}'.format(prex,f + '_emb_mean',i))\n",
    "\n",
    "\n",
    "\n",
    "    joblib.dump(res[[prex] +feature ],'w2v_feat/{}_{}_emb.pkl'.format(prex,f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8711774d-35a2-4621-93b0-8e8f1475008a",
   "metadata": {},
   "outputs": [],
   "source": [
    "piddf['title'] = piddf['title'].apply(lambda x:x.split(' '))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5dd0601-6c24-4529-bba6-1899bd03621a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2957/2957 [00:00<00:00, 26731.38it/s]\n"
     ]
    }
   ],
   "source": [
    "generate_w2v_feat(piddf[['id','title']],'id','title',32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b32e7d6e-c356-4339-8399-7ec146bd34c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2957/2957 [00:00<00:00, 3751.03it/s]\n"
     ]
    }
   ],
   "source": [
    "piddf['abstract'] = piddf['abstract'].apply(lambda x:x.lower())\n",
    "piddf['abstract'] = piddf['abstract'].apply(lambda x:x.split(' '))\n",
    "generate_w2v_feat(piddf,'id','abstract',32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96759e25-fe14-42db-8157-6beeef80c140",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2957/2957 [00:00<00:00, 34748.16it/s]\n"
     ]
    }
   ],
   "source": [
    "piddf['keywords'] = piddf['keywords'].apply(lambda x:(' '.join(x)).lower())\n",
    "piddf['keywords'] = piddf['keywords'].apply(lambda x:x.split(' '))\n",
    "generate_w2v_feat(piddf,'id','keywords',32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c1b6a05-c659-4b7b-af22-7e6e83f21b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2957/2957 [00:00<00:00, 44515.20it/s]\n"
     ]
    }
   ],
   "source": [
    "piddf['venue'] = piddf['venue'].apply(lambda x:x.split(' '))\n",
    "generate_w2v_feat(piddf,'id','venue',32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8ae6faa-914c-46d1-9575-c9401f80a0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:01<00:00, 10.98it/s]\n"
     ]
    }
   ],
   "source": [
    "temp = joblib.load('w2v_feat/id_title_emb.pkl')\n",
    "\n",
    "pid_title_dict = dict(zip(temp['id'],temp[temp.columns[1:]].values))\n",
    "mean_vec = temp[temp.columns[1:]].values.mean(axis = 0)\n",
    "\n",
    "def cos_similarity(target, embedding):\n",
    "    numerator = np.sum(target * embedding, axis=1)\n",
    "    denominator = np.sqrt(np.sum(np.square(target)) * np.sum(np.square(embedding),axis=1))\n",
    "    return numerator / denominator\n",
    "ans = []\n",
    "for autherID in tqdm(data['autherID'].unique()):\n",
    "    pid_list = data[data['autherID'] == autherID]['PID'].to_list()\n",
    "    for f1 in pid_list:\n",
    "        x1 = pid_title_dict.get(f1,mean_vec)\n",
    "        x2 = [f for f in pid_list if f != f1]\n",
    "        x2 = [pid_title_dict.get(f,mean_vec) for f in x2]\n",
    "        ans.append(list(cos_similarity(x1, x2)))\n",
    "data['title_w2v_sim'] = ans\n",
    "\n",
    "\n",
    "for f in ['title_w2v_sim']:\n",
    "    data[f + '_mean'] = data[f].apply(lambda x:np.mean(x))\n",
    "    data[f + '_max'] = data[f].apply(lambda x:np.max(x))\n",
    "    data[f + '_min'] = data[f].apply(lambda x:np.min(x))\n",
    "    data[f + '_std'] = data[f].apply(lambda x:np.std(x))\n",
    "    data[f + '_median'] = data[f].apply(lambda x:np.median(x))\n",
    "data = data.drop(['autherName'],axis = 1)\n",
    "\n",
    "data.to_feather('feats/title_w2v_sim.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42052503-8ca0-44cb-a9f7-973e6d84f7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:01<00:00, 10.84it/s]\n"
     ]
    }
   ],
   "source": [
    "data = pd.concat([train,valid,test]).reset_index(drop = True)\n",
    "temp = joblib.load('w2v_feat/id_abstract_emb.pkl')\n",
    "\n",
    "pid_title_dict = dict(zip(temp['id'],temp[temp.columns[1:]].values))\n",
    "mean_vec = temp[temp.columns[1:]].values.mean(axis = 0)\n",
    "\n",
    "def cos_similarity(target, embedding):\n",
    "    numerator = np.sum(target * embedding, axis=1)\n",
    "    denominator = np.sqrt(np.sum(np.square(target)) * np.sum(np.square(embedding),axis=1))\n",
    "    return numerator / denominator\n",
    "ans = []\n",
    "for autherID in tqdm(data['autherID'].unique()):\n",
    "    pid_list = data[data['autherID'] == autherID]['PID'].to_list()\n",
    "    for f1 in pid_list:\n",
    "        x1 = pid_title_dict.get(f1,mean_vec)\n",
    "        x2 = [f for f in pid_list if f != f1]\n",
    "        x2 = [pid_title_dict.get(f,mean_vec) for f in x2]\n",
    "        ans.append(list(cos_similarity(x1, x2)))\n",
    "data['abstract_w2v_sim'] = ans\n",
    "\n",
    "\n",
    "for f in ['abstract_w2v_sim']:\n",
    "    data[f + '_mean'] = data[f].apply(lambda x:np.mean(x))\n",
    "    data[f + '_max'] = data[f].apply(lambda x:np.max(x))\n",
    "    data[f + '_min'] = data[f].apply(lambda x:np.min(x))\n",
    "    data[f + '_std'] = data[f].apply(lambda x:np.std(x))\n",
    "    data[f + '_median'] = data[f].apply(lambda x:np.median(x))\n",
    "data = data.drop(['autherName'],axis = 1)\n",
    "\n",
    "data.to_feather('feats/abstract_w2v_sim.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cc9745-97d3-4c37-a7a0-6126e824fd4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb898c4f-b66e-4c7b-b953-720e42f7f6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:01<00:00, 10.84it/s]\n"
     ]
    }
   ],
   "source": [
    "data = pd.concat([train,valid,test]).reset_index(drop = True)\n",
    "temp = joblib.load('w2v_feat/id_keywords_emb.pkl')\n",
    "\n",
    "pid_title_dict = dict(zip(temp['id'],temp[temp.columns[1:]].values))\n",
    "mean_vec = temp[temp.columns[1:]].values.mean(axis = 0)\n",
    "\n",
    "def cos_similarity(target, embedding):\n",
    "    numerator = np.sum(target * embedding, axis=1)\n",
    "    denominator = np.sqrt(np.sum(np.square(target)) * np.sum(np.square(embedding),axis=1))\n",
    "    return numerator / denominator\n",
    "ans = []\n",
    "for autherID in tqdm(data['autherID'].unique()):\n",
    "    pid_list = data[data['autherID'] == autherID]['PID'].to_list()\n",
    "    for f1 in pid_list:\n",
    "        x1 = pid_title_dict.get(f1,mean_vec)\n",
    "        x2 = [f for f in pid_list if f != f1]\n",
    "        x2 = [pid_title_dict.get(f,mean_vec) for f in x2]\n",
    "        ans.append(list(cos_similarity(x1, x2)))\n",
    "data['keywords_w2v_sim'] = ans\n",
    "\n",
    "\n",
    "for f in ['keywords_w2v_sim']:\n",
    "    data[f + '_mean'] = data[f].apply(lambda x:np.mean(x))\n",
    "    data[f + '_max'] = data[f].apply(lambda x:np.max(x))\n",
    "    data[f + '_min'] = data[f].apply(lambda x:np.min(x))\n",
    "    data[f + '_std'] = data[f].apply(lambda x:np.std(x))\n",
    "    data[f + '_median'] = data[f].apply(lambda x:np.median(x))\n",
    "data = data.drop(['autherName'],axis = 1)\n",
    "\n",
    "data.to_feather('feats/keywords_w2v_sim.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7588133a-b675-4cc2-882b-b86c97f96595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "301b672b-648a-427e-ab68-ee6b6e48e9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:01<00:00, 10.85it/s]\n"
     ]
    }
   ],
   "source": [
    "data = pd.concat([train,valid,test]).reset_index(drop = True)\n",
    "temp = joblib.load('w2v_feat/id_venue_emb.pkl')\n",
    "\n",
    "pid_title_dict = dict(zip(temp['id'],temp[temp.columns[1:]].values))\n",
    "mean_vec = temp[temp.columns[1:]].values.mean(axis = 0)\n",
    "\n",
    "def cos_similarity(target, embedding):\n",
    "    numerator = np.sum(target * embedding, axis=1)\n",
    "    denominator = np.sqrt(np.sum(np.square(target)) * np.sum(np.square(embedding),axis=1))\n",
    "    return numerator / denominator\n",
    "ans = []\n",
    "for autherID in tqdm(data['autherID'].unique()):\n",
    "    pid_list = data[data['autherID'] == autherID]['PID'].to_list()\n",
    "    for f1 in pid_list:\n",
    "        x1 = pid_title_dict.get(f1,mean_vec)\n",
    "        x2 = [f for f in pid_list if f != f1]\n",
    "        x2 = [pid_title_dict.get(f,mean_vec) for f in x2]\n",
    "        ans.append(list(cos_similarity(x1, x2)))\n",
    "data['venue_w2v_sim'] = ans\n",
    "\n",
    "\n",
    "for f in ['venue_w2v_sim']:\n",
    "    data[f + '_mean'] = data[f].apply(lambda x:np.mean(x))\n",
    "    data[f + '_max'] = data[f].apply(lambda x:np.max(x))\n",
    "    data[f + '_min'] = data[f].apply(lambda x:np.min(x))\n",
    "    data[f + '_std'] = data[f].apply(lambda x:np.std(x))\n",
    "    data[f + '_median'] = data[f].apply(lambda x:np.median(x))\n",
    "data = data.drop(['autherName'],axis = 1)\n",
    "\n",
    "data.to_feather('feats/venue_w2v_sim.feather')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
