{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7bbe5ed-cf9e-479d-971f-5cf20ac79d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold,GroupKFold\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import re\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import json\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e7a0c3d-0ed5-46e1-af99-0ecfc9ca63ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../IND-WhoIsWho/pid_to_info_all.json', 'r') as file:\n",
    "    pid = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a1cff79-23ee-46a0-a9b4-ad5a7693a41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train = pd.read_feather('data/train.feather')\n",
    "valid = pd.read_feather('data/valid.feather')\n",
    "test = pd.read_feather('data/test.feather')\n",
    "\n",
    "train['dataset_mode'] = 0\n",
    "valid['dataset_mode'] = 1\n",
    "test['dataset_mode'] = 2\n",
    "\n",
    "data = pd.concat([train,valid,test]).reset_index(drop = True)\n",
    "\n",
    "data['id'] = data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ce0049b-91ee-4de5-bbfb-d4920cd5bb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_feather('temp_data/autherID_authors.feather')\n",
    "df = df[df['flag']!=1].reset_index(drop = True)\n",
    "df = df.merge(data[['autherID','PID','id']],on = ['autherID','PID'],how = 'left')\n",
    "t_df = df[df['auther_authors_name_count']>=2].reset_index(drop = True)\n",
    "#t_df = t_df[t_df['label'].notna()].reset_index(drop = True)\n",
    "t_df = t_df.groupby(['autherID','authors_name'])['PID'].agg(list).reset_index()\n",
    "t_df = t_df[t_df['PID'].apply(lambda x:len(x)>1)].reset_index(drop = True)\n",
    "t_df['PID'] = t_df['PID'].apply(lambda x:'  '.join(sorted(x)))\n",
    "t_df = t_df.drop_duplicates(subset = ['PID'],keep = 'first').reset_index(drop = True)\n",
    "t_df['PID'] = t_df['PID'].apply(lambda x:x.split('  ' ))\n",
    "t_df1 = t_df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "564dd1be-2d1f-40e7-b0bb-0ad3403af67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2957/2957 [00:00<00:00, 271040.82it/s]\n"
     ]
    }
   ],
   "source": [
    "data['keywords'] = data['PID'].apply(lambda x:pid[x]['keywords'])\n",
    "\n",
    "ans = []\n",
    "for auther,keywords,PID, in tqdm(data[['autherID','keywords','PID']].values):\n",
    "    if len(keywords)>0 and type(keywords)== list and keywords[0] !='null':\n",
    "        for i in range(len(keywords)):\n",
    "            ans.append([auther,keywords[i],PID])\n",
    "            \n",
    "df = pd.DataFrame(ans,columns = ['autherID','keyword','PID'])\n",
    "df['auther_keyword_count'] = df.groupby(['autherID','keyword'])['PID'].transform('count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "980b3a4a-86f6-4711-b59b-98e68db704ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2957/2957 [00:00<00:00, 308199.32it/s]\n"
     ]
    }
   ],
   "source": [
    "data['keywords'] = data['PID'].apply(lambda x:pid[x]['keywords'])\n",
    "\n",
    "ans = []\n",
    "for auther,keywords,PID, in tqdm(data[['autherID','keywords','PID']].values):\n",
    "    if len(keywords)>0 and type(keywords)== list and keywords[0] !='null':\n",
    "        for i in range(len(keywords)):\n",
    "            ans.append([auther,keywords[i],PID])\n",
    "            \n",
    "df = pd.DataFrame(ans,columns = ['autherID','keyword','PID'])\n",
    "df['auther_keyword_count'] = df.groupby(['autherID','keyword'])['PID'].transform('count')\n",
    "\n",
    "t_df = df[df['auther_keyword_count']>=2].reset_index(drop = True)\n",
    "#t_df = t_df[t_df['label'].notna()].reset_index(drop = True)\n",
    "t_df = t_df.groupby(['autherID','keyword'])['PID'].agg(list).reset_index()\n",
    "t_df = t_df[t_df['PID'].apply(lambda x:len(x)>1)].reset_index(drop = True)\n",
    "t_df['PID'] = t_df['PID'].apply(lambda x:'  '.join(sorted(x)))\n",
    "t_df = t_df.drop_duplicates(subset = ['PID'],keep = 'first').reset_index(drop = True)\n",
    "t_df['PID'] = t_df['PID'].apply(lambda x:x.split('  ' ))\n",
    "t_df2 = t_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a00b2cfa-b776-4299-afad-822dab00e3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_clean(x,mode = 1):\n",
    "    x = x.lower()\n",
    "    for f in list('-?=～—/_？）￥:#\\\\\\'.》”^>$]}|+)、（&{`《,(%!“<’\"】；【‘~*@…：，。[;') :\n",
    "        x = x.replace(f,'')\n",
    "        \n",
    "    if mode == 'venue':\n",
    "        number_pattern = r'\\d+'\n",
    "        x = re.sub(number_pattern, '', x)\n",
    "    \n",
    "    \n",
    "    for i in range(3):\n",
    "        x = x.replace('  ',' ')\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = pd.concat([train,valid,test]).reset_index(drop = True)\n",
    "\n",
    "\n",
    "data['pid_title'] = data['PID'].apply(lambda x:pid[x]['title'])\n",
    "data['abstract'] = data['PID'].apply(lambda x:pid[x]['abstract'])\n",
    "data['keywords'] = data['PID'].apply(lambda x:pid[x]['keywords'])\n",
    "data['venue'] = data['PID'].apply(lambda x:pid[x]['venue'])\n",
    "data['year'] = data['PID'].apply(lambda x:pid[x]['year'])\n",
    "data['authors'] = data['PID'].apply(lambda x:pid[x]['authors'])\n",
    "\n",
    "data['venue'] = data['venue'].fillna('')\n",
    "data['venue'] = data['venue'].apply(lambda x:text_clean(x,mode = 'venue'))\n",
    "\"\"\"\n",
    "data['venue_id'] = data['venue'].map(dict(temp.values))\n",
    "\n",
    "data['venue'] = data['venue'].apply(lambda x:x.lower())\n",
    "temp = data.groupby(['autherID','venue_id'])['label','year','PID'].agg(list).reset_index()\n",
    "temp = temp[temp['label'].apply(lambda x:len(x) >1)]\n",
    "#temp = temp[temp['venue'] != '']\n",
    "t_df3 = temp.copy()\n",
    "\"\"\"\n",
    "data['venue'] = data['venue'].apply(lambda x:x.lower())\n",
    "temp = data.groupby(['autherID','venue'])['label','year','PID'].agg(list).reset_index()\n",
    "temp = temp[temp['label'].apply(lambda x:len(x) >1)]\n",
    "temp = temp[temp['venue'] != '']\n",
    "t_df3 = temp.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c810f8d8-7455-49cb-8aa6-2ba390f6a047",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_feather('temp_data/autherID_authors.feather')\n",
    "df = df[df['authors_org'] != ''].reset_index(drop= True)\n",
    "df = df[df['flag'] ==1].reset_index(drop= True)\n",
    "\n",
    "df = df.sort_values(['autherID','auther_authors_name_count'],ascending = False).reset_index(drop = True)\n",
    "df = df.groupby(['autherID','PID']).head(1).reset_index(drop = True)\n",
    "\n",
    "df = df.merge(data[['autherID','PID','label','year']],on = ['autherID','PID'],how = 'left')\n",
    "\n",
    "temp = df.groupby(['autherID','authors_org'])['label','year','PID'].agg(list).reset_index()\n",
    "temp = temp[temp['label'].apply(lambda x:len(x)>1)]\n",
    "t_df4 = temp.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b124da9-d8ba-4363-aa4f-37e6d6e27ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_year(t_df1):\n",
    "    \n",
    "    t_dict = dict(zip(data['autherID'] +'_'+ data['PID'].to_list(),data['year'].to_list()))\n",
    "    t_df1['year'] = t_df1.apply(lambda x:[t_dict[x['autherID'] + '_' +  f] for f in x['PID']],axis = 1)\n",
    "\n",
    "    t_dict = dict(zip(data['autherID'] +'_'+ data['PID'].to_list(),data['label'].to_list()))\n",
    "    t_df1['label'] = t_df1.apply(lambda x:[t_dict[x['autherID'] + '_' +  f] for f in x['PID']],axis = 1)\n",
    "\n",
    "    def get_sort_pid(a,b):\n",
    "        ans = []\n",
    "        for x,y in zip(a,b):\n",
    "            if y == '':\n",
    "                y =0\n",
    "            ans.append([str(x),int(y)])\n",
    "\n",
    "        ans =  sorted(ans,key = lambda x:x[1])\n",
    "\n",
    "        res = []\n",
    "        temp = []\n",
    "        for x,y in ans:\n",
    "            if len(temp) == 0 or y-temp[-1][1]<=1:\n",
    "                temp.append([x,y])\n",
    "            else:\n",
    "\n",
    "                res.append( temp.copy())\n",
    "                temp = []\n",
    "                temp.append([x,y])\n",
    "        if len(temp) != 0:\n",
    "            res.append(temp.copy())\n",
    "\n",
    "        return res\n",
    "\n",
    "    t_df1['pid_split'] = t_df1.apply(lambda x:get_sort_pid(x['PID'],x['year']),axis = 1)\n",
    "\n",
    "    ans = []\n",
    "    for i in range(t_df1.shape[0]):\n",
    "\n",
    "        tt = t_df1.loc[i]\n",
    "        temp_pid_split = tt['pid_split'].copy()\n",
    "\n",
    "        if len(temp_pid_split) <=1:\n",
    "            tt['pid_split'] = temp_pid_split[0]\n",
    "            ans.append(tt.copy().values)\n",
    "        else:\n",
    "\n",
    "            for xx in temp_pid_split:\n",
    "\n",
    "                tt['pid_split'] = xx\n",
    "                ans.append(tt.copy().values)\n",
    "\n",
    "    t_df1_1 = pd.DataFrame(ans,columns = t_df1.columns)\n",
    "    t_df1_1['PID'] = t_df1_1['pid_split'].apply(lambda x:[f[0] for f in x])\n",
    "    t_df1_1['year'] = t_df1_1['pid_split'].apply(lambda x:[f[1] for f in x])\n",
    "    t_df1_1 = t_df1_1[t_df1_1['year'].apply(lambda x:len(x)>1)]\n",
    "\n",
    "    return t_df1_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c88b1884-0ce9-43da-8d7d-634ae2716f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_df_1_1 = get_split_year(t_df1)\n",
    "t_df_2_1 = get_split_year(t_df2)\n",
    "t_df_3_1 = get_split_year(t_df3.reset_index(drop = True))\n",
    "t_df_4_1 = get_split_year(t_df4.reset_index(drop = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33e250df-63c6-4928-a4cd-129d13cdd5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['id'] = data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f46ac3be-4b15-45fd-af02-b605aec1db2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_dict = dict(zip(data['autherID'] +'_'+ data['PID'].to_list(),data['id'].to_list()))\n",
    "t_df_1_1['id'] = t_df_1_1.apply(lambda x:[t_dict[x['autherID'] + '_' +  f] for f in x['PID']],axis = 1)\n",
    "t_df_2_1['id'] = t_df_2_1.apply(lambda x:[t_dict[x['autherID'] + '_' +  f] for f in x['PID']],axis = 1)\n",
    "t_df_3_1['id'] = t_df_3_1.apply(lambda x:[t_dict[x['autherID'] + '_' +  f] for f in x['PID']],axis = 1)\n",
    "t_df_4_1['id'] = t_df_4_1.apply(lambda x:[t_dict[x['autherID'] + '_' +  f] for f in x['PID']],axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32a42d9c-1806-4727-af51-1f4d45c90e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_engerate(x_list):\n",
    "    res = []\n",
    "    x_list = sorted(x_list)\n",
    "    for i in range(len(x_list)):\n",
    "        for j in range(i+1,len(x_list)):\n",
    "            res.append((x_list[i],x_list[j]))\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e024b2d1-8ed3-4977-844d-32bea2989164",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_df_1_1['id'] = t_df_1_1['id'].map(edge_engerate)\n",
    "edges_1_1 = t_df_1_1['id'].to_list()\n",
    "edges_1_1 = np.concatenate(edges_1_1)\n",
    "\n",
    "\n",
    "\n",
    "t_df_2_1['id'] = t_df_2_1['id'].map(edge_engerate)\n",
    "edges_2_1 = t_df_2_1['id'].to_list()\n",
    "edges_2_1 = np.concatenate(edges_2_1)\n",
    "\n",
    "\n",
    "t_df_3_1['id'] = t_df_3_1['id'].map(edge_engerate)\n",
    "edges_3_1 = t_df_3_1['id'].to_list()\n",
    "edges_3_1 = np.concatenate(edges_3_1)\n",
    "\n",
    "\n",
    "t_df_4_1['id'] = t_df_4_1['id'].map(edge_engerate)\n",
    "edges_4_1 = t_df_4_1['id'].to_list()\n",
    "edges_4_1 = np.concatenate(edges_4_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "013a690b-203f-4db0-bfe2-f87e815055e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['edges_all.pkl']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#edges = pd.concat([pd.DataFrame(edges_1_1),pd.DataFrame(edges_2),pd.DataFrame(edges_3),pd.DataFrame(edges_4)])\n",
    "edges = pd.concat([pd.DataFrame(edges_1_1),pd.DataFrame(edges_2_1),pd.DataFrame(edges_3_1),pd.DataFrame(edges_4_1)])\n",
    "#edges = pd.concat([pd.DataFrame(edges_4_1)])\n",
    "\n",
    "#edges = pd.concat([pd.DataFrame(edges_1_1)])\n",
    "edges = edges.drop_duplicates().reset_index(drop= True)\n",
    "edges = edges.sort_values(0).reset_index(drop= True)\n",
    "\n",
    "joblib.dump(edges,'edges_all.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986f75c8-e38f-4a25-b654-04ae9eef8681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00646060-d4e1-4a11-ab96-c04b75ea59d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "308ac51f-5761-4efa-99bc-4b86190a6329",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_feather('all_feat.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0e70349c-6fe8-4f41-97fe-13384abdcbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feats = set(joblib.load('sub/feat_select_cat_feats.pkl'))| set(joblib.load('sub/feat_select_lgb_feats.pkl'))| set(joblib.load('sub/feat_select_xgb_feats.pkl')) | set(data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6ca338f9-9bd4-48c2-ab27-12035572459e",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = joblib.load('edges_all.pkl')\n",
    "data['id'] = data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e3de6c39-c6a9-4ede-9e45-d29c4cf5bff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_0 = dict(edges.groupby(0)[1].agg(list) )\n",
    "edges_1 = dict(edges.groupby(1)[0].agg(list) )\n",
    "df = data[['autherID','PID','label','id']]\n",
    "df['id_0'] = df['id'].map(edges_0)\n",
    "df['id_1'] = df['id'].map(edges_1)\n",
    "df.loc[df['id_0'].isna(),'id_0']  ='[]'\n",
    "df.loc[df['id_1'].isna(),'id_1']  ='[]'\n",
    "df.loc[df['id_0'] == '[]','id_0']  =df.loc[df['id_0'] == '[]','id_0'].apply(lambda x:eval(x))\n",
    "df.loc[df['id_1'] =='[]' ,'id_1']  =df.loc[df['id_1'] == '[]','id_1'].apply(lambda x:eval(x))\n",
    "df['n_id'] = (df['id_0'] + df['id_1']).apply(lambda x:list(set(x)))\n",
    "df['n_id'] = df['n_id'] + df['id'].apply(lambda x:[x])\n",
    "df = df.drop(['id_0','id_1'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "526e36e6-5189-49ce-9477-b421e425b594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:04<00:00,  8.15it/s]\n"
     ]
    }
   ],
   "source": [
    "group_feat =list(set(['_'.join(f.split('_')[2:-1]) for f in list(all_feats) if f.startswith('group_') and f != 'group_feat_cnt']))\n",
    "for feat in tqdm(group_feat):\n",
    "    t_dict = dict(data[['id',feat]].values)\n",
    "    df[feat] = df['n_id'].apply(lambda x:[t_dict[f] for f in x])\n",
    "    df[f'group_feat_{feat}_mean'] = df[feat].apply(lambda x:np.mean(x))\n",
    "    df[f'group_feat_{feat}_std'] = df[feat].apply(lambda x:np.std(x))\n",
    "    df[f'group_feat_{feat}_cnt'] = df[feat].apply(lambda x:len(x))\n",
    "    del df[feat]\n",
    "df[f'group_feat_cnt'] = df['n_id'].apply(lambda x:len(x))\n",
    "df  =df.drop(['id','n_id','label'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4129b615-3637-4bea-901d-558e9948cf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = data.merge(df,on = ['autherID','PID'],how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "adf48ed9-c3cb-4dbf-9cee-626b0fcf223c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_feather('all_feats2.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463978c2-aace-479e-9177-e250d1c24af6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
