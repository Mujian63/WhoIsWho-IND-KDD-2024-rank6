{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83739d27-d39f-49cd-9f0c-bae759874a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold,GroupKFold\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import re\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import json\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d946b40-3c96-4c6c-b5fb-8928a0a90006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold,GroupKFold,KFold\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import re\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import os\n",
    "\n",
    "from datasets import concatenate_datasets,load_dataset,load_from_disk\n",
    "from transformers import AutoModel, AutoTokenizer, AdamW, DataCollatorWithPadding\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pytorch_lightning as pytl\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import polars as pl\n",
    "from torch_geometric.nn import GCNConv,SAGEConv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d936deb-764d-4c10-94f8-cdd702f4e5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../IND-WhoIsWho/pid_to_info_all.json', 'r') as file:\n",
    "    pid = json.load(file)\n",
    "train = pd.read_feather('data/train.feather')\n",
    "valid = pd.read_feather('data/valid.feather')\n",
    "test = pd.read_feather('data/test.feather')\n",
    "\n",
    "piddf = joblib.load('data/pid_df.pkl')\n",
    "\n",
    "data = pd.concat([train,valid,test]).reset_index(drop = True)\n",
    "piddf = piddf[piddf['id'].isin(set(data['PID']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ece5d9bd-3ec5-409e-ad07-199625de18df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61502595-ffbd-47ae-be2e-0ec3bcafa77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "def l2_normalize(vector):\n",
    "    norm = np.linalg.norm(vector, ord=2)\n",
    "    if norm == 0:\n",
    "        return vector\n",
    "    return vector / norm\n",
    "\n",
    "bge_m3_tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-m3')\n",
    "bge_m3_model = AutoModel.from_pretrained('BAAI/bge-m3')\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24a609e8-45ec-43cf-a3cc-5569730cea6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bge_m3_text2vec(text):\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    encoded_input = bge_m3_tokenizer(text, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "    bge_m3_model_input = bge_m3_model.to(device)\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = bge_m3_model_input(**encoded_input)\n",
    "\n",
    "    # Perform pooling. In this case, mean pooling.\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Convert to a list and return as JSON response\n",
    "    embeddings_list = sentence_embeddings.tolist()\n",
    "    # normalize handle\n",
    "    normalized_embeddings_list = l2_normalize(np.array(embeddings_list))\n",
    "    normalized_embeddings_list.tolist()\n",
    "    return normalized_embeddings_list.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddbbbaa4-ae1b-4c16-bdd2-a79e4420c996",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 93/93 [00:10<00:00,  9.07it/s]\n"
     ]
    }
   ],
   "source": [
    "piddf['title'] = piddf['title'].apply(lambda x:x.lower())\n",
    "piddf = piddf.reset_index(drop = True)\n",
    "\n",
    "title_emb = []\n",
    "batch_size = 32\n",
    "for i in tqdm(range(piddf.shape[0]//batch_size + 1)):\n",
    "    text = piddf[i*32:(i+1)*32]['title'].to_list()\n",
    "    title_emb.append(bge_m3_text2vec(text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6856a1c-bf26-4d4a-815a-2ca69ae3a765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bge_m3_emb/title2vec_dict.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_emb = np.concatenate(title_emb)\n",
    "t_dict = dict(zip(piddf['id'].to_list(),title_emb))\n",
    "joblib.dump(t_dict,'bge_m3_emb/title2vec_dict.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dce815-371f-415d-ba7f-12d3eca60108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45a2eda9-9f0a-434b-99c6-16e71b86cf93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:05<00:00,  2.60it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def cos_similarity(target, embedding):\n",
    "    numerator = np.sum(target * embedding, axis=1)\n",
    "    denominator = np.sqrt(np.sum(np.square(target)) * np.sum(np.square(embedding),axis=1))\n",
    "    return numerator / denominator\n",
    "\n",
    "ans = []\n",
    "for autherID in tqdm(data['autherID'].unique()):\n",
    "    pid_list = data[data['autherID'] == autherID]['PID'].to_list()\n",
    "    for f1 in pid_list:\n",
    "        x1 = t_dict.get(f1)\n",
    "        x2 = [f for f in pid_list if f != f1]\n",
    "        x2 = [t_dict.get(f) for f in x2]\n",
    "        ans.append(list(cos_similarity(x1, x2)))\n",
    "data['pid_title_sim'] = ans\n",
    "\n",
    "for f in ['pid_title_sim']:\n",
    "    data[f + '_mean'] = data[f].apply(lambda x:np.mean(x))\n",
    "    data[f + '_max'] = data[f].apply(lambda x:np.max(x))\n",
    "    data[f + '_min'] = data[f].apply(lambda x:np.min(x))\n",
    "    data[f + '_std'] = data[f].apply(lambda x:np.std(x))\n",
    "    data[f + '_median'] = data[f].apply(lambda x:np.median(x))\n",
    "    \n",
    "data = data.drop(['pid_title_sim'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9621081-5aca-4dde-bfcb-064eea8222c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['PID', 'autherID','pid_title_sim_mean',\n",
    "       'pid_title_sim_max', 'pid_title_sim_min', 'pid_title_sim_std',\n",
    "       'pid_title_sim_median']].to_feather('bge_m3_emb_title_feat.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ee25ba-950b-45ae-879b-a89e8d702c94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce39eae4-4647-4f89-9986-202868a8092c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c626a986-0e1a-4c14-be9a-c6df106c91a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05ce4e75-b9e1-49db-b29f-ef2cb14a389c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_unidecode import unidecode\n",
    "from typing import Dict, List, Tuple\n",
    "import codecs\n",
    "def replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n",
    "    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n",
    "\n",
    "\n",
    "def replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n",
    "    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n",
    "\n",
    "# Register the encoding and decoding error handlers for `utf-8` and `cp1252`.\n",
    "codecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\n",
    "codecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n",
    "\n",
    "def resolve_encodings_and_normalize(text: str) -> str:\n",
    "    \"\"\"Resolve the encoding problems and normalize the abnormal characters.\"\"\"\n",
    "    text = (\n",
    "        text.replace('\\r\\n',' [SEP]').replace('\\n',' [BR]').replace(\"\\'\",\"'\")\n",
    "        .encode(\"raw_unicode_escape\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "    )\n",
    "    text = unidecode(text)\n",
    "    return text\n",
    "\n",
    "def resolve_encodings_and_normalize(text: str) -> str:\n",
    "    try:\n",
    "        \"\"\"Resolve the encoding problems and normalize the abnormal characters.\"\"\"\n",
    "        text = (\n",
    "            text.replace('\\n',' ').replace(\"\\'\",\"'\")\n",
    "            .encode(\"raw_unicode_escape\")\n",
    "            .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "            .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n",
    "            .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "        )\n",
    "        text = unidecode(text)\n",
    "        return text\n",
    "    except:\n",
    "        return ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c79e806-e3e1-4c95-bb56-0cd5b1ea5023",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 740/740 [00:44<00:00, 16.74it/s]\n"
     ]
    }
   ],
   "source": [
    "piddf['abstract'] = piddf['abstract'].apply(lambda x:x.lower())\n",
    "piddf.loc[i*4+2,'abstract'] = ''\n",
    "piddf['abstract'] = piddf['abstract'].map(resolve_encodings_and_normalize)\n",
    "piddf['abstract'] = piddf['abstract'].apply(lambda x:x[:2048])\n",
    "piddf = piddf.reset_index(drop = True)\n",
    "abstract_emb = []\n",
    "batch_size = 4\n",
    "for i in tqdm(range(piddf.shape[0]//batch_size + 1)):\n",
    "    text = piddf[i*batch_size:(i+1)*batch_size]['abstract'].to_list()\n",
    "    abstract_emb.append(bge_m3_text2vec(text))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d151c68-73f3-4b7d-a75d-43ac8b9cc90a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bge_m3_emb/abstract2vec_dict.pkl']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract_emb = np.concatenate(abstract_emb)\n",
    "t_dict = dict(zip(piddf['id'].to_list(),abstract_emb))\n",
    "joblib.dump(t_dict,'bge_m3_emb/abstract2vec_dict.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dee96b65-4a1e-41d5-938b-ec677584ebc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:06<00:00,  2.50it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def cos_similarity(target, embedding):\n",
    "    numerator = np.sum(target * embedding, axis=1)\n",
    "    denominator = np.sqrt(np.sum(np.square(target)) * np.sum(np.square(embedding),axis=1))\n",
    "    return numerator / denominator\n",
    "\n",
    "ans = []\n",
    "for autherID in tqdm(data['autherID'].unique()):\n",
    "    pid_list = data[data['autherID'] == autherID]['PID'].to_list()\n",
    "    for f1 in pid_list:\n",
    "        x1 = t_dict.get(f1)\n",
    "        x2 = [f for f in pid_list if f != f1]\n",
    "        x2 = [t_dict.get(f) for f in x2]\n",
    "        ans.append(list(cos_similarity(x1, x2)))\n",
    "data['pid_abstract_sim'] = ans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f1d7d45-bf9c-4bc9-b5fd-ec0cd54c7252",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in ['pid_abstract_sim']:\n",
    "    data[f + '_mean'] = data[f].apply(lambda x:np.mean(x))\n",
    "    data[f + '_max'] = data[f].apply(lambda x:np.max(x))\n",
    "    data[f + '_min'] = data[f].apply(lambda x:np.min(x))\n",
    "    data[f + '_std'] = data[f].apply(lambda x:np.std(x))\n",
    "    data[f + '_median'] = data[f].apply(lambda x:np.median(x))\n",
    "\n",
    "#data = data.drop(['pid_title_sim'],axis = 1)\n",
    "data = data.drop(['autherName'],axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cccdd2a8-3e1e-4283-8d23-3792e2ac61f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['PID', 'autherID','pid_abstract_sim_mean',\n",
    "       'pid_abstract_sim_max', 'pid_abstract_sim_min', 'pid_abstract_sim_std',\n",
    "       'pid_abstract_sim_median']].to_feather('feats/bge_m3_abstract_sim_feats.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12f1122-40f4-42db-9a01-19272f53c5fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
