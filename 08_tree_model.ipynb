{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17eea647-e068-40e8-bb1a-5868361174bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold,GroupKFold\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import re\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import json\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12f16a37-1b9f-468e-92d6-3e09d0ca2e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../IND-WhoIsWho/pid_to_info_all.json', 'r') as file:\n",
    "    pid = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "909ecc77-0d04-41ef-80c6-72995a0ff406",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_feather('all_feat.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e123fbf-5b99-40f1-b659-39a24b5ec8ab",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372\n",
      "['bge_m3_pid_title_sim_mean', 'bge_m3_pid_title_sim_max', 'bge_m3_pid_title_sim_min', 'bge_m3_pid_title_sim_std', 'bge_m3_pid_title_sim_median', 'bge_m3_pid_abstract_sim_mean', 'bge_m3_pid_abstract_sim_max', 'bge_m3_pid_abstract_sim_min', 'bge_m3_pid_abstract_sim_std', 'bge_m3_pid_abstract_sim_median', 'tfidf_pid_abstract_sim_mean', 'tfidf_pid_abstract_sim_max', 'tfidf_pid_abstract_sim_min', 'tfidf_pid_abstract_sim_std', 'tfidf_pid_abstract_sim_median', 'tfidf_pid_title_sim_mean', 'tfidf_pid_title_sim_max', 'tfidf_pid_title_sim_min', 'tfidf_pid_title_sim_std', 'tfidf_pid_title_sim_median', 'tfidf_pid_venue_sim_mean', 'tfidf_pid_venue_sim_max', 'tfidf_pid_venue_sim_min', 'tfidf_pid_venue_sim_std', 'tfidf_pid_venue_sim_median', 'tfidf_pid_keyword_text_sim_mean', 'tfidf_pid_keyword_text_sim_max', 'tfidf_pid_keyword_text_sim_min', 'tfidf_pid_keyword_text_sim_std', 'tfidf_pid_keyword_text_sim_median', 'jaccard_pid_keyword_cnt_1_mean', 'jaccard_pid_keyword_cnt_1_max', 'jaccard_pid_keyword_cnt_1_min', 'jaccard_pid_keyword_cnt_1_std', 'jaccard_pid_keyword_cnt_1_median', 'jaccard_pid_keyword_cnt_2_mean', 'jaccard_pid_keyword_cnt_2_max', 'jaccard_pid_keyword_cnt_2_min', 'jaccard_pid_keyword_cnt_2_std', 'jaccard_pid_keyword_cnt_2_median', 'jaccard_pid_keyword_cnt_3_mean', 'jaccard_pid_keyword_cnt_3_max', 'jaccard_pid_keyword_cnt_3_min', 'jaccard_pid_keyword_cnt_3_std', 'jaccard_pid_keyword_cnt_3_median', 'jaccard_pid_auther_cnt_1_mean', 'jaccard_pid_auther_cnt_1_max', 'jaccard_pid_auther_cnt_1_min', 'jaccard_pid_auther_cnt_1_std', 'jaccard_pid_auther_cnt_2_mean', 'jaccard_pid_auther_cnt_2_max', 'jaccard_pid_auther_cnt_2_min', 'jaccard_pid_auther_cnt_2_std', 'jaccard_pid_auther_cnt_3_mean', 'jaccard_pid_auther_cnt_3_max', 'jaccard_pid_auther_cnt_3_min', 'jaccard_pid_auther_cnt_3_std', 'jaccard_pid_autherraw_cnt_1_mean', 'jaccard_pid_autherraw_cnt_1_max', 'jaccard_pid_autherraw_cnt_1_min', 'jaccard_pid_autherraw_cnt_1_std', 'jaccard_pid_autherraw_cnt_2_mean', 'jaccard_pid_autherraw_cnt_2_max', 'jaccard_pid_autherraw_cnt_2_min', 'jaccard_pid_autherraw_cnt_2_std', 'jaccard_pid_autherraw_cnt_3_mean', 'jaccard_pid_autherraw_cnt_3_max', 'jaccard_pid_autherraw_cnt_3_min', 'jaccard_pid_autherraw_cnt_3_std', 'abstract_tfidf_word_0', 'abstract_tfidf_word_1', 'abstract_tfidf_word_2', 'abstract_tfidf_word_3', 'abstract_tfidf_word_4', 'abstract_tfidf_word_5', 'abstract_tfidf_word_6', 'abstract_tfidf_word_7', 'abstract_tfidf_word_8', 'abstract_tfidf_word_9', 'abstract_tfidf_word_10', 'abstract_tfidf_word_11', 'abstract_tfidf_word_12', 'abstract_tfidf_word_13', 'abstract_tfidf_word_14', 'abstract_tfidf_word_15', 'abstract_tfidf_word_16', 'abstract_tfidf_word_17', 'abstract_tfidf_word_18', 'abstract_tfidf_word_19', 'abstract_tfidf_word_20', 'abstract_tfidf_word_21', 'abstract_tfidf_word_22', 'abstract_tfidf_word_23', 'abstract_tfidf_word_24', 'abstract_tfidf_word_25', 'abstract_tfidf_word_26', 'abstract_tfidf_word_27', 'abstract_tfidf_word_28', 'abstract_tfidf_word_29', 'abstract_tfidf_word_30', 'abstract_tfidf_word_31', 'abstract_countvec_0', 'abstract_countvec_1', 'abstract_countvec_2', 'abstract_countvec_3', 'abstract_countvec_4', 'abstract_countvec_5', 'abstract_countvec_6', 'abstract_countvec_7', 'abstract_countvec_8', 'abstract_countvec_9', 'abstract_countvec_10', 'abstract_countvec_11', 'abstract_countvec_12', 'abstract_countvec_13', 'abstract_countvec_14', 'abstract_countvec_15', 'abstract_countvec_16', 'abstract_countvec_17', 'abstract_countvec_18', 'abstract_countvec_19', 'abstract_countvec_20', 'abstract_countvec_21', 'abstract_countvec_22', 'abstract_countvec_23', 'abstract_countvec_24', 'abstract_countvec_25', 'abstract_countvec_26', 'abstract_countvec_27', 'abstract_countvec_28', 'abstract_countvec_29', 'abstract_countvec_30', 'abstract_countvec_31', 'title_tfidf_word_0', 'title_tfidf_word_1', 'title_tfidf_word_2', 'title_tfidf_word_3', 'title_tfidf_word_4', 'title_tfidf_word_5', 'title_tfidf_word_6', 'title_tfidf_word_7', 'title_tfidf_word_8', 'title_tfidf_word_9', 'title_tfidf_word_10', 'title_tfidf_word_11', 'title_tfidf_word_12', 'title_tfidf_word_13', 'title_tfidf_word_14', 'title_tfidf_word_15', 'title_tfidf_word_16', 'title_tfidf_word_17', 'title_tfidf_word_18', 'title_tfidf_word_19', 'title_tfidf_word_20', 'title_tfidf_word_21', 'title_tfidf_word_22', 'title_tfidf_word_23', 'title_tfidf_word_24', 'title_tfidf_word_25', 'title_tfidf_word_26', 'title_tfidf_word_27', 'title_tfidf_word_28', 'title_tfidf_word_29', 'title_tfidf_word_30', 'title_tfidf_word_31', 'title_countvec_0', 'title_countvec_1', 'title_countvec_2', 'title_countvec_3', 'title_countvec_4', 'title_countvec_5', 'title_countvec_6', 'title_countvec_7', 'title_countvec_8', 'title_countvec_9', 'title_countvec_10', 'title_countvec_11', 'title_countvec_12', 'title_countvec_13', 'title_countvec_14', 'title_countvec_15', 'title_countvec_16', 'title_countvec_17', 'title_countvec_18', 'title_countvec_19', 'title_countvec_20', 'title_countvec_21', 'title_countvec_22', 'title_countvec_23', 'title_countvec_24', 'title_countvec_25', 'title_countvec_26', 'title_countvec_27', 'title_countvec_28', 'title_countvec_29', 'title_countvec_30', 'title_countvec_31', 'venue_tfidf_word_0', 'venue_tfidf_word_1', 'venue_tfidf_word_2', 'venue_tfidf_word_3', 'venue_tfidf_word_4', 'venue_tfidf_word_5', 'venue_tfidf_word_6', 'venue_tfidf_word_7', 'venue_tfidf_word_8', 'venue_tfidf_word_9', 'venue_tfidf_word_10', 'venue_tfidf_word_11', 'venue_tfidf_word_12', 'venue_tfidf_word_13', 'venue_tfidf_word_14', 'venue_tfidf_word_15', 'venue_countvec_0', 'venue_countvec_1', 'venue_countvec_2', 'venue_countvec_3', 'venue_countvec_4', 'venue_countvec_5', 'venue_countvec_6', 'venue_countvec_7', 'venue_countvec_8', 'venue_countvec_9', 'venue_countvec_10', 'venue_countvec_11', 'venue_countvec_12', 'venue_countvec_13', 'venue_countvec_14', 'venue_countvec_15', 'keyword_text_tfidf_word_0', 'keyword_text_tfidf_word_1', 'keyword_text_tfidf_word_2', 'keyword_text_tfidf_word_3', 'keyword_text_tfidf_word_4', 'keyword_text_tfidf_word_5', 'keyword_text_tfidf_word_6', 'keyword_text_tfidf_word_7', 'keyword_text_tfidf_word_8', 'keyword_text_tfidf_word_9', 'keyword_text_tfidf_word_10', 'keyword_text_tfidf_word_11', 'keyword_text_tfidf_word_12', 'keyword_text_tfidf_word_13', 'keyword_text_tfidf_word_14', 'keyword_text_tfidf_word_15', 'keyword_text_countvec_0', 'keyword_text_countvec_1', 'keyword_text_countvec_2', 'keyword_text_countvec_3', 'keyword_text_countvec_4', 'keyword_text_countvec_5', 'keyword_text_countvec_6', 'keyword_text_countvec_7', 'keyword_text_countvec_8', 'keyword_text_countvec_9', 'keyword_text_countvec_10', 'keyword_text_countvec_11', 'keyword_text_countvec_12', 'keyword_text_countvec_13', 'keyword_text_countvec_14', 'keyword_text_countvec_15', 'autherID_keyword_emb_016', 'autherID_keyword_emb_116', 'autherID_keyword_emb_216', 'autherID_keyword_emb_316', 'autherID_keyword_emb_416', 'autherID_keyword_emb_516', 'autherID_keyword_emb_616', 'autherID_keyword_emb_716', 'autherID_keyword_emb_816', 'autherID_keyword_emb_916', 'autherID_keyword_emb_1016', 'autherID_keyword_emb_1116', 'autherID_keyword_emb_1216', 'autherID_keyword_emb_1316', 'autherID_keyword_emb_1416', 'autherID_keyword_emb_1516', 'autherID_venue_emb_08', 'autherID_venue_emb_18', 'autherID_venue_emb_28', 'autherID_venue_emb_38', 'autherID_venue_emb_48', 'autherID_venue_emb_58', 'autherID_venue_emb_68', 'autherID_venue_emb_78', 'abstract_emb_svd_0', 'abstract_emb_svd_1', 'abstract_emb_svd_2', 'abstract_emb_svd_3', 'abstract_emb_svd_4', 'abstract_emb_svd_5', 'abstract_emb_svd_6', 'abstract_emb_svd_7', 'abstract_emb_svd_8', 'abstract_emb_svd_9', 'abstract_emb_svd_10', 'abstract_emb_svd_11', 'abstract_emb_svd_12', 'abstract_emb_svd_13', 'abstract_emb_svd_14', 'abstract_emb_svd_15', 'title_emb_svd_0', 'title_emb_svd_1', 'title_emb_svd_2', 'title_emb_svd_3', 'title_emb_svd_4', 'title_emb_svd_5', 'title_emb_svd_6', 'title_emb_svd_7', 'title_emb_svd_8', 'title_emb_svd_9', 'title_emb_svd_10', 'title_emb_svd_11', 'title_emb_svd_12', 'title_emb_svd_13', 'title_emb_svd_14', 'title_emb_svd_15', 'year', 'author_rank_ratio', 'author_rank', 'abstract_w2v_sim_mean', 'abstract_w2v_sim_max', 'abstract_w2v_sim_min', 'abstract_w2v_sim_std', 'abstract_w2v_sim_median', 'title_w2v_sim_mean', 'title_w2v_sim_max', 'title_w2v_sim_min', 'title_w2v_sim_std', 'title_w2v_sim_median', 'chatglm_pid_title_sim_mean', 'chatglm_pid_title_sim_max', 'chatglm_pid_title_sim_min', 'chatglm_pid_title_sim_std', 'chatglm_pid_title_sim_median', 'chatglm_pid_abstract_sim_mean', 'chatglm_pid_abstract_sim_max', 'chatglm_pid_abstract_sim_min', 'chatglm_pid_abstract_sim_std', 'chatglm_pid_abstract_sim_median', 'chatglm_title_emb_svd_0', 'chatglm_title_emb_svd_1', 'chatglm_title_emb_svd_2', 'chatglm_title_emb_svd_3', 'chatglm_title_emb_svd_4', 'chatglm_title_emb_svd_5', 'chatglm_title_emb_svd_6', 'chatglm_title_emb_svd_7', 'chatglm_title_emb_svd_8', 'chatglm_title_emb_svd_9', 'chatglm_title_emb_svd_10', 'chatglm_title_emb_svd_11', 'chatglm_title_emb_svd_12', 'chatglm_title_emb_svd_13', 'chatglm_title_emb_svd_14', 'chatglm_title_emb_svd_15', 'chatglm_abstract_emb_svd_0', 'chatglm_abstract_emb_svd_1', 'chatglm_abstract_emb_svd_2', 'chatglm_abstract_emb_svd_3', 'chatglm_abstract_emb_svd_4', 'chatglm_abstract_emb_svd_5', 'chatglm_abstract_emb_svd_6', 'chatglm_abstract_emb_svd_7', 'chatglm_abstract_emb_svd_8', 'chatglm_abstract_emb_svd_9', 'chatglm_abstract_emb_svd_10', 'chatglm_abstract_emb_svd_11', 'chatglm_abstract_emb_svd_12', 'chatglm_abstract_emb_svd_13', 'chatglm_abstract_emb_svd_14', 'chatglm_abstract_emb_svd_15']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "df_train = data[data['label'].notna()].reset_index(drop = True)\n",
    "df_test = data[data['label'].isna()].reset_index(drop = True)\n",
    "df_train['label'] = 1 - df_train['label']\n",
    "\n",
    "drop_feat =['tfidf_autherName','dataset_mode','id'] + ['PID','label','autherID','pid_title','autherName', 'abstract', 'keywords', 'venue', 'authors', 'index','cos_sim_pid','autherName_count'] \n",
    "used_feat = [f for f in df_train.columns if f not in (['PID','label','autherID'] + drop_feat )]\n",
    "print(len(used_feat))\n",
    "print(used_feat)\n",
    "ycol = 'label'\n",
    "\n",
    "#del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a40019ca-3561-4caa-90c6-f0d22468baef",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "372\n",
      "================2020==============\n",
      "\n",
      "Fold_1 Training ================================\n",
      "\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "\n",
      "Fold_2 Training ================================\n",
      "\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "\n",
      "Fold_3 Training ================================\n",
      "\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "\n",
      "Fold_4 Training ================================\n",
      "\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "\n",
      "Fold_5 Training ================================\n",
      "\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "\n",
      "Fold_6 Training ================================\n",
      "\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input data must be 2 dimensional and non empty.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m#df_train.iloc[val_idx,'fold_id'] = fold_id\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFold_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m Training ================================\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(fold_id \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 82\u001b[0m lgb_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mused_feat\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m                  \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m[\u001b[49m\u001b[43mused_feat\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m                  \u001b[49m\u001b[38;5;66;43;03m#eval_metric = 'auc',\u001b[39;49;00m\n\u001b[1;32m     85\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m                  \u001b[49m\u001b[38;5;66;43;03m#early_stopping_rounds=200\u001b[39;49;00m\n\u001b[1;32m     87\u001b[0m \u001b[43m                      \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m pred_val \u001b[38;5;241m=\u001b[39m lgb_model\u001b[38;5;241m.\u001b[39mpredict_proba(X_val[used_feat])[:,\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     94\u001b[0m df_oof\u001b[38;5;241m.\u001b[39mloc[val_idx, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mycol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_prob\u001b[39m\u001b[38;5;124m\"\u001b[39m  ] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m pred_val\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/lightgbm/sklearn.py:967\u001b[0m, in \u001b[0;36mLGBMClassifier.fit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    965\u001b[0m             valid_sets[i] \u001b[38;5;241m=\u001b[39m (valid_x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_le\u001b[38;5;241m.\u001b[39mtransform(valid_y))\n\u001b[0;32m--> 967\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_class_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_class_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/lightgbm/sklearn.py:748\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    745\u001b[0m evals_result \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    746\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[0;32m--> 748\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evals_result:\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evals_result \u001b[38;5;241m=\u001b[39m evals_result\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/lightgbm/engine.py:275\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    273\u001b[0m         booster\u001b[38;5;241m.\u001b[39mset_train_data_name(train_data_name)\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m valid_set, name_valid_set \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(reduced_valid_sets, name_valid_sets):\n\u001b[0;32m--> 275\u001b[0m         \u001b[43mbooster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_valid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname_valid_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m     train_set\u001b[38;5;241m.\u001b[39m_reverse_update_params()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/lightgbm/basic.py:2935\u001b[0m, in \u001b[0;36mBooster.add_valid\u001b[0;34m(self, data, name)\u001b[0m\n\u001b[1;32m   2930\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39m_predictor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__init_predictor:\n\u001b[1;32m   2931\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdd validation data failed, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2932\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou should use same predictor for these data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2933\u001b[0m _safe_call(_LIB\u001b[38;5;241m.\u001b[39mLGBM_BoosterAddValidData(\n\u001b[1;32m   2934\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m-> 2935\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mhandle))\n\u001b[1;32m   2936\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_sets\u001b[38;5;241m.\u001b[39mappend(data)\n\u001b[1;32m   2937\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname_valid_sets\u001b[38;5;241m.\u001b[39mappend(name)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/lightgbm/basic.py:1784\u001b[0m, in \u001b[0;36mDataset.construct\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1781\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_params(reference_params)\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mused_indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1783\u001b[0m     \u001b[38;5;66;03m# create valid\u001b[39;00m\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1786\u001b[0m \u001b[43m                    \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predictor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1787\u001b[0m \u001b[43m                    \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1789\u001b[0m     \u001b[38;5;66;03m# construct subset\u001b[39;00m\n\u001b[1;32m   1790\u001b[0m     used_indices \u001b[38;5;241m=\u001b[39m list_to_1d_numpy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mused_indices, np\u001b[38;5;241m.\u001b[39mint32, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mused_indices\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/lightgbm/basic.py:1474\u001b[0m, in \u001b[0;36mDataset._lazy_init\u001b[0;34m(self, data, label, reference, weight, group, init_score, predictor, silent, feature_name, categorical_feature, params)\u001b[0m\n\u001b[1;32m   1472\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpandas_categorical \u001b[38;5;241m=\u001b[39m reference\u001b[38;5;241m.\u001b[39mpandas_categorical\n\u001b[1;32m   1473\u001b[0m     categorical_feature \u001b[38;5;241m=\u001b[39m reference\u001b[38;5;241m.\u001b[39mcategorical_feature\n\u001b[0;32m-> 1474\u001b[0m data, feature_name, categorical_feature, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpandas_categorical \u001b[38;5;241m=\u001b[39m \u001b[43m_data_from_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1475\u001b[0m \u001b[43m                                                                                     \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1476\u001b[0m \u001b[43m                                                                                     \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1477\u001b[0m \u001b[43m                                                                                     \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpandas_categorical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1478\u001b[0m label \u001b[38;5;241m=\u001b[39m _label_from_pandas(label)\n\u001b[1;32m   1480\u001b[0m \u001b[38;5;66;03m# process for args\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/lightgbm/basic.py:566\u001b[0m, in \u001b[0;36m_data_from_pandas\u001b[0;34m(data, feature_name, categorical_feature, pandas_categorical)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd_DataFrame):\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 566\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput data must be 2 dimensional and non empty.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m feature_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m feature_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    568\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Input data must be 2 dimensional and non empty."
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import StratifiedGroupKFold  # type: ignore\n",
    "\n",
    "\n",
    "df_importance_dict = dict()\n",
    "df_oof = df_train[['PID','autherID', 'label']].copy()\n",
    "prediction = df_test[['PID','autherID']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#iter_dict = dict({'CHANNEL_A':4000,'CHANNEL_B':10000,'CHANNEL_C':2500})\n",
    "for ycol in ['label']:\n",
    "#for ycol in [ 'CHANNEL_C']:\n",
    "\n",
    "    print(ycol)\n",
    "    #try:\n",
    "    #    used_feat = feature_dict[ycol].copy()\n",
    "    #except:\n",
    "    #    used_feat = list(feature_dict[ycol])[0]\n",
    "    print(len(used_feat))\n",
    "    \n",
    "    \n",
    "    df_oof[f\"{ycol}_prob\"] = 0\n",
    "    prediction[f\"{ycol}_prob\"] = 0\n",
    "\n",
    "    \n",
    "    df_importance_list = []\n",
    "\n",
    "    folds = 10\n",
    "    seeds =[2020]#,2019,2003,2011,2017, 2027]\n",
    "    for seed in seeds:\n",
    "\n",
    "        model2 =  LGBMClassifier(objective = 'binary',boosting_type='gbdt',num_leaves=34,max_depth=15,learning_rate=0.01,n_estimators=3500,subsample=0.8,\n",
    "                               feature_fraction=0.8,reg_alpha=10,reg_lambda= 12,random_state=seed,is_unbalance=True,metric='logloss',n_jobs=24,bagging_freq=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #model2 = XGBClassifier(boosting_type='gbdt',num_leaves=15,max_depth=6,learning_rate=0.01,n_estimators=3500,subsample=0.8,tree_method = 'gpu_hist',\n",
    "        #                   feature_fraction=0.7,random_state=seed,is_unbalance=True,eval_metric='auc')\n",
    "        \"\"\"\n",
    "        ctb_params = dict(iterations=3000,\n",
    "                          learning_rate=0.01,\n",
    "                          depth=8,\n",
    "                          #cat_features = [],\n",
    "                          l2_leaf_reg=30,\n",
    "                          bootstrap_type='Bernoulli',\n",
    "                          subsample=0.8,\n",
    "                          loss_function='Logloss',\n",
    "                          #eval_metric = 'AUC',\n",
    "                          metric_period=100,\n",
    "                          od_type='Iter',\n",
    "                          od_wait=30,\n",
    "                          task_type='GPU',\n",
    "                          allow_writing_files=False,\n",
    "                          )\n",
    "\n",
    "        print(\"Feature Elimination Performing.\")\n",
    "        model2 = CatBoostClassifier(**ctb_params)\n",
    "        \"\"\"\n",
    "        print(\"================{}==============\".format(seed))\n",
    "        #kfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "        kfold = StratifiedGroupKFold(n_splits=folds)\n",
    "        \n",
    "        for fold_id, (trn_idx, val_idx) in enumerate(\n",
    "                kfold.split(df_train[used_feat], df_train[ycol],groups=df_train['autherID'])):\n",
    "        #for fold_id, (trn_idx, val_idx) in enumerate(\n",
    "        #        kfold.split(df_train[used_feat], df_train[ycol],groups = df_train['autherID'])):\n",
    "            X_train = df_train.iloc[trn_idx][used_feat]\n",
    "            Y_train = df_train.iloc[trn_idx][ycol]\n",
    "\n",
    "            X_val = df_train.iloc[val_idx][used_feat]\n",
    "            Y_val = df_train.iloc[val_idx][ycol]\n",
    "            #df_train.iloc[val_idx,'fold_id'] = fold_id\n",
    "\n",
    "            print('\\nFold_{} Training ================================\\n'.format(fold_id + 1))\n",
    "\n",
    "\n",
    "            lgb_model = model2.fit(X_train[used_feat],Y_train,\n",
    "                              eval_set=[(X_val[used_feat], Y_val)],\n",
    "                              #eval_metric = 'auc',\n",
    "                              verbose=100,\n",
    "                              #early_stopping_rounds=200\n",
    "                                  )\n",
    "\n",
    "\n",
    "\n",
    "            pred_val = lgb_model.predict_proba(X_val[used_feat])[:,1]\n",
    "\n",
    "\n",
    "            df_oof.loc[val_idx, f\"{ycol}_prob\"  ] += pred_val\n",
    "\n",
    "\n",
    "            pred_test = lgb_model.predict_proba(df_test[used_feat])[:,1]\n",
    "            prediction[f\"{ycol}_prob\" ] += pred_test\n",
    "\n",
    "            df_importance = pd.DataFrame({\n",
    "                'column': used_feat,\n",
    "                'importance': lgb_model.feature_importances_,\n",
    "            })\n",
    "            df_importance_list.append(df_importance)\n",
    "            #break\n",
    "\n",
    "            del lgb_model, pred_val, pred_test, X_train, Y_train, X_val, Y_val\n",
    "            gc.collect()\n",
    "            #break\n",
    "            \n",
    "       \n",
    "        df_importance_dict[ycol] = df_importance_list.copy()\n",
    "    #break   \n",
    "LABEL_COLUME = ['label']  \n",
    "prediction[[f\"{ycol}_prob\" for ycol in LABEL_COLUME ]] = prediction[[f\"{ycol}_prob\" for ycol in LABEL_COLUME ]]/(folds*len(seeds))\n",
    "\n",
    "df_oof[[f\"{ycol}_prob\" for ycol in LABEL_COLUME ]] = df_oof[[f\"{ycol}_prob\" for ycol in LABEL_COLUME ]] / len(seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5e577d-5bab-43f2-a3be-0ca54e197599",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oof.to_feather('oof2/lgb_oof.feather')\n",
    "prediction.to_feather('oof2/lgb_prediction.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32eec4f-1b06-430f-aaf5-5ac28800df7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import StratifiedGroupKFold  # type: ignore\n",
    "\n",
    "\n",
    "df_importance_dict = dict()\n",
    "df_oof = df_train[['PID','autherID', 'label']].copy()\n",
    "prediction = df_test[['PID','autherID']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#iter_dict = dict({'CHANNEL_A':4000,'CHANNEL_B':10000,'CHANNEL_C':2500})\n",
    "for ycol in ['label']:\n",
    "#for ycol in [ 'CHANNEL_C']:\n",
    "\n",
    "    print(ycol)\n",
    "    #try:\n",
    "    #    used_feat = feature_dict[ycol].copy()\n",
    "    #except:\n",
    "    #    used_feat = list(feature_dict[ycol])[0]\n",
    "    print(len(used_feat))\n",
    "    \n",
    "    \n",
    "    df_oof[f\"{ycol}_prob\"] = 0\n",
    "    prediction[f\"{ycol}_prob\"] = 0\n",
    "\n",
    "    \n",
    "    df_importance_list = []\n",
    "\n",
    "    folds = 10\n",
    "    seeds =[2020]#,2019,2003,2011,2017, 2027]\n",
    "    for seed in seeds:\n",
    "\n",
    "        #model2 =  LGBMClassifier(objective = 'binary',boosting_type='gbdt',num_leaves=34,max_depth=15,learning_rate=0.01,n_estimators=3500,subsample=0.8,\n",
    "        #                       feature_fraction=0.8,reg_alpha=10,reg_lambda= 12,random_state=seed,is_unbalance=True,metric='logloss',n_jobs=24,bagging_freq=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        model2 = XGBClassifier(boosting_type='gbdt',num_leaves=15,max_depth=6,learning_rate=0.01,n_estimators=3500,subsample=0.8,tree_method = 'gpu_hist',\n",
    "                           feature_fraction=0.7,random_state=seed,is_unbalance=True,eval_metric='auc')\n",
    "        \"\"\"\n",
    "        ctb_params = dict(iterations=3000,\n",
    "                          learning_rate=0.01,\n",
    "                          depth=8,\n",
    "                          #cat_features = [],\n",
    "                          l2_leaf_reg=30,\n",
    "                          bootstrap_type='Bernoulli',\n",
    "                          subsample=0.8,\n",
    "                          loss_function='Logloss',\n",
    "                          #eval_metric = 'AUC',\n",
    "                          metric_period=100,\n",
    "                          od_type='Iter',\n",
    "                          od_wait=30,\n",
    "                          task_type='GPU',\n",
    "                          allow_writing_files=False,\n",
    "                          )\n",
    "\n",
    "        print(\"Feature Elimination Performing.\")\n",
    "        model2 = CatBoostClassifier(**ctb_params)\n",
    "        \"\"\"\n",
    "        print(\"================{}==============\".format(seed))\n",
    "        #kfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "        kfold = StratifiedGroupKFold(n_splits=folds)\n",
    "        \n",
    "        for fold_id, (trn_idx, val_idx) in enumerate(\n",
    "                kfold.split(df_train[used_feat], df_train[ycol],groups=df_train['autherID'])):\n",
    "        #for fold_id, (trn_idx, val_idx) in enumerate(\n",
    "        #        kfold.split(df_train[used_feat], df_train[ycol],groups = df_train['autherID'])):\n",
    "            X_train = df_train.iloc[trn_idx][used_feat]\n",
    "            Y_train = df_train.iloc[trn_idx][ycol]\n",
    "\n",
    "            X_val = df_train.iloc[val_idx][used_feat]\n",
    "            Y_val = df_train.iloc[val_idx][ycol]\n",
    "            #df_train.iloc[val_idx,'fold_id'] = fold_id\n",
    "\n",
    "            print('\\nFold_{} Training ================================\\n'.format(fold_id + 1))\n",
    "\n",
    "\n",
    "            lgb_model = model2.fit(X_train[used_feat],Y_train,\n",
    "                              eval_set=[(X_val[used_feat], Y_val)],\n",
    "                              #eval_metric = 'auc',\n",
    "                              verbose=100,\n",
    "                              #early_stopping_rounds=200\n",
    "                                  )\n",
    "\n",
    "\n",
    "\n",
    "            pred_val = lgb_model.predict_proba(X_val[used_feat])[:,1]\n",
    "\n",
    "\n",
    "            df_oof.loc[val_idx, f\"{ycol}_prob\"  ] += pred_val\n",
    "\n",
    "\n",
    "            pred_test = lgb_model.predict_proba(df_test[used_feat])[:,1]\n",
    "            prediction[f\"{ycol}_prob\" ] += pred_test\n",
    "\n",
    "            df_importance = pd.DataFrame({\n",
    "                'column': used_feat,\n",
    "                'importance': lgb_model.feature_importances_,\n",
    "            })\n",
    "            df_importance_list.append(df_importance)\n",
    "            #break\n",
    "\n",
    "            del lgb_model, pred_val, pred_test, X_train, Y_train, X_val, Y_val\n",
    "            gc.collect()\n",
    "            #break\n",
    "            \n",
    "       \n",
    "        df_importance_dict[ycol] = df_importance_list.copy()\n",
    "    #break   \n",
    "LABEL_COLUME = ['label']  \n",
    "prediction[[f\"{ycol}_prob\" for ycol in LABEL_COLUME ]] = prediction[[f\"{ycol}_prob\" for ycol in LABEL_COLUME ]]/(folds*len(seeds))\n",
    "\n",
    "df_oof[[f\"{ycol}_prob\" for ycol in LABEL_COLUME ]] = df_oof[[f\"{ycol}_prob\" for ycol in LABEL_COLUME ]] / len(seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975d235d-8095-443f-a6fd-f5e42f4e0063",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oof.to_feather('oof2/xgb_oof.feather')\n",
    "prediction.to_feather('oof2/xgb_prediction.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f3f891-4465-4ca1-b71b-15f6f5dee106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import StratifiedGroupKFold  # type: ignore\n",
    "\n",
    "\n",
    "df_importance_dict = dict()\n",
    "df_oof = df_train[['PID','autherID', 'label']].copy()\n",
    "prediction = df_test[['PID','autherID']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#iter_dict = dict({'CHANNEL_A':4000,'CHANNEL_B':10000,'CHANNEL_C':2500})\n",
    "for ycol in ['label']:\n",
    "#for ycol in [ 'CHANNEL_C']:\n",
    "\n",
    "    print(ycol)\n",
    "    #try:\n",
    "    #    used_feat = feature_dict[ycol].copy()\n",
    "    #except:\n",
    "    #    used_feat = list(feature_dict[ycol])[0]\n",
    "    print(len(used_feat))\n",
    "    \n",
    "    \n",
    "    df_oof[f\"{ycol}_prob\"] = 0\n",
    "    prediction[f\"{ycol}_prob\"] = 0\n",
    "\n",
    "    \n",
    "    df_importance_list = []\n",
    "\n",
    "    folds = 10\n",
    "    seeds =[2020]#,2019,2003,2011,2017, 2027]\n",
    "    for seed in seeds:\n",
    "\n",
    "        #model2 =  LGBMClassifier(objective = 'binary',boosting_type='gbdt',num_leaves=34,max_depth=15,learning_rate=0.01,n_estimators=3500,subsample=0.8,\n",
    "        #                       feature_fraction=0.8,reg_alpha=10,reg_lambda= 12,random_state=seed,is_unbalance=True,metric='logloss',n_jobs=24,bagging_freq=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #model2 = XGBClassifier(boosting_type='gbdt',num_leaves=15,max_depth=6,learning_rate=0.01,n_estimators=3500,subsample=0.8,tree_method = 'gpu_hist',\n",
    "        #                   feature_fraction=0.7,random_state=seed,is_unbalance=True,eval_metric='auc')\n",
    "        \n",
    "        ctb_params = dict(iterations=3000,\n",
    "                          learning_rate=0.01,\n",
    "                          depth=8,\n",
    "                          #cat_features = [],\n",
    "                          l2_leaf_reg=30,\n",
    "                          bootstrap_type='Bernoulli',\n",
    "                          subsample=0.8,\n",
    "                          loss_function='Logloss',\n",
    "                          #eval_metric = 'AUC',\n",
    "                          metric_period=100,\n",
    "                          od_type='Iter',\n",
    "                          od_wait=30,\n",
    "                          task_type='GPU',\n",
    "                          allow_writing_files=False,\n",
    "                          )\n",
    "\n",
    "        print(\"Feature Elimination Performing.\")\n",
    "        model2 = CatBoostClassifier(**ctb_params)\n",
    "        \n",
    "        print(\"================{}==============\".format(seed))\n",
    "        #kfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "        kfold = StratifiedGroupKFold(n_splits=folds)\n",
    "        \n",
    "        for fold_id, (trn_idx, val_idx) in enumerate(\n",
    "                kfold.split(df_train[used_feat], df_train[ycol],groups=df_train['autherID'])):\n",
    "        #for fold_id, (trn_idx, val_idx) in enumerate(\n",
    "        #        kfold.split(df_train[used_feat], df_train[ycol],groups = df_train['autherID'])):\n",
    "            X_train = df_train.iloc[trn_idx][used_feat]\n",
    "            Y_train = df_train.iloc[trn_idx][ycol]\n",
    "\n",
    "            X_val = df_train.iloc[val_idx][used_feat]\n",
    "            Y_val = df_train.iloc[val_idx][ycol]\n",
    "            #df_train.iloc[val_idx,'fold_id'] = fold_id\n",
    "\n",
    "            print('\\nFold_{} Training ================================\\n'.format(fold_id + 1))\n",
    "\n",
    "\n",
    "            lgb_model = model2.fit(X_train[used_feat],Y_train,\n",
    "                              eval_set=[(X_val[used_feat], Y_val)],\n",
    "                              #eval_metric = 'auc',\n",
    "                              verbose=100,\n",
    "                              #early_stopping_rounds=200\n",
    "                                  )\n",
    "\n",
    "\n",
    "\n",
    "            pred_val = lgb_model.predict_proba(X_val[used_feat])[:,1]\n",
    "\n",
    "\n",
    "            df_oof.loc[val_idx, f\"{ycol}_prob\"  ] += pred_val\n",
    "\n",
    "\n",
    "            pred_test = lgb_model.predict_proba(df_test[used_feat])[:,1]\n",
    "            prediction[f\"{ycol}_prob\" ] += pred_test\n",
    "\n",
    "            df_importance = pd.DataFrame({\n",
    "                'column': used_feat,\n",
    "                'importance': lgb_model.feature_importances_,\n",
    "            })\n",
    "            df_importance_list.append(df_importance)\n",
    "            #break\n",
    "\n",
    "            del lgb_model, pred_val, pred_test, X_train, Y_train, X_val, Y_val\n",
    "            gc.collect()\n",
    "            #break\n",
    "            \n",
    "       \n",
    "        df_importance_dict[ycol] = df_importance_list.copy()\n",
    "    #break   \n",
    "LABEL_COLUME = ['label']  \n",
    "prediction[[f\"{ycol}_prob\" for ycol in LABEL_COLUME ]] = prediction[[f\"{ycol}_prob\" for ycol in LABEL_COLUME ]]/(folds*len(seeds))\n",
    "\n",
    "df_oof[[f\"{ycol}_prob\" for ycol in LABEL_COLUME ]] = df_oof[[f\"{ycol}_prob\" for ycol in LABEL_COLUME ]] / len(seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1121200-4665-4c00-aace-093df80f085e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oof.to_feather('oof2/cat_oof.feather')\n",
    "prediction.to_feather('oof2/cat_prediction.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8410f61-bfa6-404a-985f-20095c431e26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e639dcb-baaf-4405-860b-15fd664efa7b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "475\n",
      "['bge_m3_pid_title_sim_mean', 'bge_m3_pid_title_sim_max', 'bge_m3_pid_title_sim_min', 'bge_m3_pid_title_sim_std', 'bge_m3_pid_title_sim_median', 'bge_m3_pid_abstract_sim_mean', 'bge_m3_pid_abstract_sim_max', 'bge_m3_pid_abstract_sim_min', 'bge_m3_pid_abstract_sim_std', 'bge_m3_pid_abstract_sim_median', 'tfidf_pid_abstract_sim_mean', 'tfidf_pid_abstract_sim_max', 'tfidf_pid_abstract_sim_min', 'tfidf_pid_abstract_sim_std', 'tfidf_pid_abstract_sim_median', 'tfidf_pid_title_sim_mean', 'tfidf_pid_title_sim_max', 'tfidf_pid_title_sim_min', 'tfidf_pid_title_sim_std', 'tfidf_pid_title_sim_median', 'tfidf_pid_venue_sim_mean', 'tfidf_pid_venue_sim_max', 'tfidf_pid_venue_sim_min', 'tfidf_pid_venue_sim_std', 'tfidf_pid_venue_sim_median', 'tfidf_pid_keyword_text_sim_mean', 'tfidf_pid_keyword_text_sim_max', 'tfidf_pid_keyword_text_sim_min', 'tfidf_pid_keyword_text_sim_std', 'tfidf_pid_keyword_text_sim_median', 'jaccard_pid_keyword_cnt_1_mean', 'jaccard_pid_keyword_cnt_1_max', 'jaccard_pid_keyword_cnt_1_min', 'jaccard_pid_keyword_cnt_1_std', 'jaccard_pid_keyword_cnt_1_median', 'jaccard_pid_keyword_cnt_2_mean', 'jaccard_pid_keyword_cnt_2_max', 'jaccard_pid_keyword_cnt_2_min', 'jaccard_pid_keyword_cnt_2_std', 'jaccard_pid_keyword_cnt_2_median', 'jaccard_pid_keyword_cnt_3_mean', 'jaccard_pid_keyword_cnt_3_max', 'jaccard_pid_keyword_cnt_3_min', 'jaccard_pid_keyword_cnt_3_std', 'jaccard_pid_keyword_cnt_3_median', 'jaccard_pid_auther_cnt_1_mean', 'jaccard_pid_auther_cnt_1_max', 'jaccard_pid_auther_cnt_1_min', 'jaccard_pid_auther_cnt_1_std', 'jaccard_pid_auther_cnt_2_mean', 'jaccard_pid_auther_cnt_2_max', 'jaccard_pid_auther_cnt_2_min', 'jaccard_pid_auther_cnt_2_std', 'jaccard_pid_auther_cnt_3_mean', 'jaccard_pid_auther_cnt_3_max', 'jaccard_pid_auther_cnt_3_min', 'jaccard_pid_auther_cnt_3_std', 'jaccard_pid_autherraw_cnt_1_mean', 'jaccard_pid_autherraw_cnt_1_max', 'jaccard_pid_autherraw_cnt_1_min', 'jaccard_pid_autherraw_cnt_1_std', 'jaccard_pid_autherraw_cnt_2_mean', 'jaccard_pid_autherraw_cnt_2_max', 'jaccard_pid_autherraw_cnt_2_min', 'jaccard_pid_autherraw_cnt_2_std', 'jaccard_pid_autherraw_cnt_3_mean', 'jaccard_pid_autherraw_cnt_3_max', 'jaccard_pid_autherraw_cnt_3_min', 'jaccard_pid_autherraw_cnt_3_std', 'abstract_tfidf_word_0', 'abstract_tfidf_word_1', 'abstract_tfidf_word_2', 'abstract_tfidf_word_3', 'abstract_tfidf_word_4', 'abstract_tfidf_word_5', 'abstract_tfidf_word_6', 'abstract_tfidf_word_7', 'abstract_tfidf_word_8', 'abstract_tfidf_word_9', 'abstract_tfidf_word_10', 'abstract_tfidf_word_11', 'abstract_tfidf_word_12', 'abstract_tfidf_word_13', 'abstract_tfidf_word_14', 'abstract_tfidf_word_15', 'abstract_tfidf_word_16', 'abstract_tfidf_word_17', 'abstract_tfidf_word_18', 'abstract_tfidf_word_19', 'abstract_tfidf_word_20', 'abstract_tfidf_word_21', 'abstract_tfidf_word_22', 'abstract_tfidf_word_23', 'abstract_tfidf_word_24', 'abstract_tfidf_word_25', 'abstract_tfidf_word_26', 'abstract_tfidf_word_27', 'abstract_tfidf_word_28', 'abstract_tfidf_word_29', 'abstract_tfidf_word_30', 'abstract_tfidf_word_31', 'abstract_countvec_0', 'abstract_countvec_1', 'abstract_countvec_2', 'abstract_countvec_3', 'abstract_countvec_4', 'abstract_countvec_5', 'abstract_countvec_6', 'abstract_countvec_7', 'abstract_countvec_8', 'abstract_countvec_9', 'abstract_countvec_10', 'abstract_countvec_11', 'abstract_countvec_12', 'abstract_countvec_13', 'abstract_countvec_14', 'abstract_countvec_15', 'abstract_countvec_16', 'abstract_countvec_17', 'abstract_countvec_18', 'abstract_countvec_19', 'abstract_countvec_20', 'abstract_countvec_21', 'abstract_countvec_22', 'abstract_countvec_23', 'abstract_countvec_24', 'abstract_countvec_25', 'abstract_countvec_26', 'abstract_countvec_27', 'abstract_countvec_28', 'abstract_countvec_29', 'abstract_countvec_30', 'abstract_countvec_31', 'title_tfidf_word_0', 'title_tfidf_word_1', 'title_tfidf_word_2', 'title_tfidf_word_3', 'title_tfidf_word_4', 'title_tfidf_word_5', 'title_tfidf_word_6', 'title_tfidf_word_7', 'title_tfidf_word_8', 'title_tfidf_word_9', 'title_tfidf_word_10', 'title_tfidf_word_11', 'title_tfidf_word_12', 'title_tfidf_word_13', 'title_tfidf_word_14', 'title_tfidf_word_15', 'title_tfidf_word_16', 'title_tfidf_word_17', 'title_tfidf_word_18', 'title_tfidf_word_19', 'title_tfidf_word_20', 'title_tfidf_word_21', 'title_tfidf_word_22', 'title_tfidf_word_23', 'title_tfidf_word_24', 'title_tfidf_word_25', 'title_tfidf_word_26', 'title_tfidf_word_27', 'title_tfidf_word_28', 'title_tfidf_word_29', 'title_tfidf_word_30', 'title_tfidf_word_31', 'title_countvec_0', 'title_countvec_1', 'title_countvec_2', 'title_countvec_3', 'title_countvec_4', 'title_countvec_5', 'title_countvec_6', 'title_countvec_7', 'title_countvec_8', 'title_countvec_9', 'title_countvec_10', 'title_countvec_11', 'title_countvec_12', 'title_countvec_13', 'title_countvec_14', 'title_countvec_15', 'title_countvec_16', 'title_countvec_17', 'title_countvec_18', 'title_countvec_19', 'title_countvec_20', 'title_countvec_21', 'title_countvec_22', 'title_countvec_23', 'title_countvec_24', 'title_countvec_25', 'title_countvec_26', 'title_countvec_27', 'title_countvec_28', 'title_countvec_29', 'title_countvec_30', 'title_countvec_31', 'venue_tfidf_word_0', 'venue_tfidf_word_1', 'venue_tfidf_word_2', 'venue_tfidf_word_3', 'venue_tfidf_word_4', 'venue_tfidf_word_5', 'venue_tfidf_word_6', 'venue_tfidf_word_7', 'venue_tfidf_word_8', 'venue_tfidf_word_9', 'venue_tfidf_word_10', 'venue_tfidf_word_11', 'venue_tfidf_word_12', 'venue_tfidf_word_13', 'venue_tfidf_word_14', 'venue_tfidf_word_15', 'venue_countvec_0', 'venue_countvec_1', 'venue_countvec_2', 'venue_countvec_3', 'venue_countvec_4', 'venue_countvec_5', 'venue_countvec_6', 'venue_countvec_7', 'venue_countvec_8', 'venue_countvec_9', 'venue_countvec_10', 'venue_countvec_11', 'venue_countvec_12', 'venue_countvec_13', 'venue_countvec_14', 'venue_countvec_15', 'keyword_text_tfidf_word_0', 'keyword_text_tfidf_word_1', 'keyword_text_tfidf_word_2', 'keyword_text_tfidf_word_3', 'keyword_text_tfidf_word_4', 'keyword_text_tfidf_word_5', 'keyword_text_tfidf_word_6', 'keyword_text_tfidf_word_7', 'keyword_text_tfidf_word_8', 'keyword_text_tfidf_word_9', 'keyword_text_tfidf_word_10', 'keyword_text_tfidf_word_11', 'keyword_text_tfidf_word_12', 'keyword_text_tfidf_word_13', 'keyword_text_tfidf_word_14', 'keyword_text_tfidf_word_15', 'keyword_text_countvec_0', 'keyword_text_countvec_1', 'keyword_text_countvec_2', 'keyword_text_countvec_3', 'keyword_text_countvec_4', 'keyword_text_countvec_5', 'keyword_text_countvec_6', 'keyword_text_countvec_7', 'keyword_text_countvec_8', 'keyword_text_countvec_9', 'keyword_text_countvec_10', 'keyword_text_countvec_11', 'keyword_text_countvec_12', 'keyword_text_countvec_13', 'keyword_text_countvec_14', 'keyword_text_countvec_15', 'autherID_keyword_emb_016', 'autherID_keyword_emb_116', 'autherID_keyword_emb_216', 'autherID_keyword_emb_316', 'autherID_keyword_emb_416', 'autherID_keyword_emb_516', 'autherID_keyword_emb_616', 'autherID_keyword_emb_716', 'autherID_keyword_emb_816', 'autherID_keyword_emb_916', 'autherID_keyword_emb_1016', 'autherID_keyword_emb_1116', 'autherID_keyword_emb_1216', 'autherID_keyword_emb_1316', 'autherID_keyword_emb_1416', 'autherID_keyword_emb_1516', 'autherID_venue_emb_08', 'autherID_venue_emb_18', 'autherID_venue_emb_28', 'autherID_venue_emb_38', 'autherID_venue_emb_48', 'autherID_venue_emb_58', 'autherID_venue_emb_68', 'autherID_venue_emb_78', 'abstract_emb_svd_0', 'abstract_emb_svd_1', 'abstract_emb_svd_2', 'abstract_emb_svd_3', 'abstract_emb_svd_4', 'abstract_emb_svd_5', 'abstract_emb_svd_6', 'abstract_emb_svd_7', 'abstract_emb_svd_8', 'abstract_emb_svd_9', 'abstract_emb_svd_10', 'abstract_emb_svd_11', 'abstract_emb_svd_12', 'abstract_emb_svd_13', 'abstract_emb_svd_14', 'abstract_emb_svd_15', 'title_emb_svd_0', 'title_emb_svd_1', 'title_emb_svd_2', 'title_emb_svd_3', 'title_emb_svd_4', 'title_emb_svd_5', 'title_emb_svd_6', 'title_emb_svd_7', 'title_emb_svd_8', 'title_emb_svd_9', 'title_emb_svd_10', 'title_emb_svd_11', 'title_emb_svd_12', 'title_emb_svd_13', 'title_emb_svd_14', 'title_emb_svd_15', 'year', 'author_rank_ratio', 'author_rank', 'abstract_w2v_sim_mean', 'abstract_w2v_sim_max', 'abstract_w2v_sim_min', 'abstract_w2v_sim_std', 'abstract_w2v_sim_median', 'title_w2v_sim_mean', 'title_w2v_sim_max', 'title_w2v_sim_min', 'title_w2v_sim_std', 'title_w2v_sim_median', 'chatglm_pid_title_sim_mean', 'chatglm_pid_title_sim_max', 'chatglm_pid_title_sim_min', 'chatglm_pid_title_sim_std', 'chatglm_pid_title_sim_median', 'chatglm_pid_abstract_sim_mean', 'chatglm_pid_abstract_sim_max', 'chatglm_pid_abstract_sim_min', 'chatglm_pid_abstract_sim_std', 'chatglm_pid_abstract_sim_median', 'chatglm_title_emb_svd_0', 'chatglm_title_emb_svd_1', 'chatglm_title_emb_svd_2', 'chatglm_title_emb_svd_3', 'chatglm_title_emb_svd_4', 'chatglm_title_emb_svd_5', 'chatglm_title_emb_svd_6', 'chatglm_title_emb_svd_7', 'chatglm_title_emb_svd_8', 'chatglm_title_emb_svd_9', 'chatglm_title_emb_svd_10', 'chatglm_title_emb_svd_11', 'chatglm_title_emb_svd_12', 'chatglm_title_emb_svd_13', 'chatglm_title_emb_svd_14', 'chatglm_title_emb_svd_15', 'chatglm_abstract_emb_svd_0', 'chatglm_abstract_emb_svd_1', 'chatglm_abstract_emb_svd_2', 'chatglm_abstract_emb_svd_3', 'chatglm_abstract_emb_svd_4', 'chatglm_abstract_emb_svd_5', 'chatglm_abstract_emb_svd_6', 'chatglm_abstract_emb_svd_7', 'chatglm_abstract_emb_svd_8', 'chatglm_abstract_emb_svd_9', 'chatglm_abstract_emb_svd_10', 'chatglm_abstract_emb_svd_11', 'chatglm_abstract_emb_svd_12', 'chatglm_abstract_emb_svd_13', 'chatglm_abstract_emb_svd_14', 'chatglm_abstract_emb_svd_15', 'group_feat_autherID_keyword_emb_1016_mean', 'group_feat_autherID_keyword_emb_1016_std', 'group_feat_autherID_keyword_emb_1016_cnt', 'group_feat_autherID_venue_emb_78_mean', 'group_feat_autherID_venue_emb_78_std', 'group_feat_autherID_venue_emb_78_cnt', 'group_feat_bge_m3_pid_title_sim_min_mean', 'group_feat_bge_m3_pid_title_sim_min_std', 'group_feat_bge_m3_pid_title_sim_min_cnt', 'group_feat_autherID_keyword_emb_116_mean', 'group_feat_autherID_keyword_emb_116_std', 'group_feat_autherID_keyword_emb_116_cnt', 'group_feat_jaccard_pid_auther_cnt_1_std_mean', 'group_feat_jaccard_pid_auther_cnt_1_std_std', 'group_feat_jaccard_pid_auther_cnt_1_std_cnt', 'group_feat_bge_m3_pid_title_sim_std_mean', 'group_feat_bge_m3_pid_title_sim_std_std', 'group_feat_bge_m3_pid_title_sim_std_cnt', 'group_feat_title_emb_svd_3_mean', 'group_feat_title_emb_svd_3_std', 'group_feat_title_emb_svd_3_cnt', 'group_feat_autherID_venue_emb_28_mean', 'group_feat_autherID_venue_emb_28_std', 'group_feat_autherID_venue_emb_28_cnt', 'group_feat_autherID_keyword_emb_516_mean', 'group_feat_autherID_keyword_emb_516_std', 'group_feat_autherID_keyword_emb_516_cnt', 'group_feat_autherID_venue_emb_68_mean', 'group_feat_autherID_venue_emb_68_std', 'group_feat_autherID_venue_emb_68_cnt', 'group_feat_bge_m3_pid_title_sim_median_mean', 'group_feat_bge_m3_pid_title_sim_median_std', 'group_feat_bge_m3_pid_title_sim_median_cnt', 'group_feat_autherID_venue_emb_58_mean', 'group_feat_autherID_venue_emb_58_std', 'group_feat_autherID_venue_emb_58_cnt', 'group_feat_autherID_keyword_emb_416_mean', 'group_feat_autherID_keyword_emb_416_std', 'group_feat_autherID_keyword_emb_416_cnt', 'group_feat_autherID_venue_emb_38_mean', 'group_feat_autherID_venue_emb_38_std', 'group_feat_autherID_venue_emb_38_cnt', 'group_feat_jaccard_pid_auther_cnt_1_min_mean', 'group_feat_jaccard_pid_auther_cnt_1_min_std', 'group_feat_jaccard_pid_auther_cnt_1_min_cnt', 'group_feat_jaccard_pid_keyword_cnt_2_std_mean', 'group_feat_jaccard_pid_keyword_cnt_2_std_std', 'group_feat_jaccard_pid_keyword_cnt_2_std_cnt', 'group_feat_title_emb_svd_1_mean', 'group_feat_title_emb_svd_1_std', 'group_feat_title_emb_svd_1_cnt', 'group_feat_autherID_venue_emb_08_mean', 'group_feat_autherID_venue_emb_08_std', 'group_feat_autherID_venue_emb_08_cnt', 'group_feat_autherID_keyword_emb_1516_mean', 'group_feat_autherID_keyword_emb_1516_std', 'group_feat_autherID_keyword_emb_1516_cnt', 'group_feat_year_mean', 'group_feat_year_std', 'group_feat_year_cnt', 'group_feat_autherID_keyword_emb_716_mean', 'group_feat_autherID_keyword_emb_716_std', 'group_feat_autherID_keyword_emb_716_cnt', 'group_feat_title_emb_svd_2_mean', 'group_feat_title_emb_svd_2_std', 'group_feat_title_emb_svd_2_cnt', 'group_feat_bge_m3_pid_title_sim_max_mean', 'group_feat_bge_m3_pid_title_sim_max_std', 'group_feat_bge_m3_pid_title_sim_max_cnt', 'group_feat_title_emb_svd_0_mean', 'group_feat_title_emb_svd_0_std', 'group_feat_title_emb_svd_0_cnt', 'group_feat_jaccard_pid_keyword_cnt_2_max_mean', 'group_feat_jaccard_pid_keyword_cnt_2_max_std', 'group_feat_jaccard_pid_keyword_cnt_2_max_cnt', 'group_feat_jaccard_pid_autherraw_cnt_3_min_mean', 'group_feat_jaccard_pid_autherraw_cnt_3_min_std', 'group_feat_jaccard_pid_autherraw_cnt_3_min_cnt', 'group_feat_autherID_venue_emb_18_mean', 'group_feat_autherID_venue_emb_18_std', 'group_feat_autherID_venue_emb_18_cnt', 'group_feat_bge_m3_pid_abstract_sim_std_mean', 'group_feat_bge_m3_pid_abstract_sim_std_std', 'group_feat_bge_m3_pid_abstract_sim_std_cnt', 'group_feat_jaccard_pid_auther_cnt_1_mean_mean', 'group_feat_jaccard_pid_auther_cnt_1_mean_std', 'group_feat_jaccard_pid_auther_cnt_1_mean_cnt', 'group_feat_jaccard_pid_autherraw_cnt_1_mean_mean', 'group_feat_jaccard_pid_autherraw_cnt_1_mean_std', 'group_feat_jaccard_pid_autherraw_cnt_1_mean_cnt', 'group_feat_jaccard_pid_autherraw_cnt_2_max_mean', 'group_feat_jaccard_pid_autherraw_cnt_2_max_std', 'group_feat_jaccard_pid_autherraw_cnt_2_max_cnt', 'group_feat_jaccard_pid_auther_cnt_3_min_mean', 'group_feat_jaccard_pid_auther_cnt_3_min_std', 'group_feat_jaccard_pid_auther_cnt_3_min_cnt', 'group_feat_autherID_keyword_emb_916_mean', 'group_feat_autherID_keyword_emb_916_std', 'group_feat_autherID_keyword_emb_916_cnt', 'group_feat_tfidf_pid_keyword_text_sim_mean_mean', 'group_feat_tfidf_pid_keyword_text_sim_mean_std', 'group_feat_tfidf_pid_keyword_text_sim_mean_cnt', 'group_feat_cnt']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1207"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_feather('all_feats2.feather')\n",
    "\n",
    "\n",
    "df_train = data[data['label'].notna()].reset_index(drop = True)\n",
    "df_test = data[data['label'].isna()].reset_index(drop = True)\n",
    "df_train['label'] = 1 - df_train['label']\n",
    "\n",
    "drop_feat =['tfidf_autherName','dataset_mode','id'] + ['PID','label','autherID','pid_title','autherName', 'abstract', 'keywords', 'venue', 'authors', 'index','cos_sim_pid','autherName_count'] \n",
    "used_feat = [f for f in df_train.columns if f not in (['PID','label','autherID'] + drop_feat )]\n",
    "print(len(used_feat))\n",
    "print(used_feat)\n",
    "ycol = 'label'\n",
    "\n",
    "#del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f6c4e5c-b1c3-4749-8005-46fe26d01aff",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "136\n",
      "================2020==============\n",
      "\n",
      "Fold_1 Training ================================\n",
      "\n",
      "\n",
      "Fold_2 Training ================================\n",
      "\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "\n",
      "Fold_3 Training ================================\n",
      "\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "\n",
      "Fold_4 Training ================================\n",
      "\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "\n",
      "Fold_5 Training ================================\n",
      "\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "\n",
      "Fold_6 Training ================================\n",
      "\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input data must be 2 dimensional and non empty.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m#df_train.iloc[val_idx,'fold_id'] = fold_id\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFold_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m Training ================================\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(fold_id \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 84\u001b[0m lgb_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mused_feat\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m                  \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m[\u001b[49m\u001b[43mused_feat\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m                  \u001b[49m\u001b[38;5;66;43;03m#eval_metric = 'auc',\u001b[39;49;00m\n\u001b[1;32m     87\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m                  \u001b[49m\u001b[38;5;66;43;03m#early_stopping_rounds=200\u001b[39;49;00m\n\u001b[1;32m     89\u001b[0m \u001b[43m                      \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m pred_val \u001b[38;5;241m=\u001b[39m lgb_model\u001b[38;5;241m.\u001b[39mpredict_proba(X_val[used_feat])[:,\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     96\u001b[0m df_oof\u001b[38;5;241m.\u001b[39mloc[val_idx, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mycol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_prob\u001b[39m\u001b[38;5;124m\"\u001b[39m  ] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m pred_val\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/lightgbm/sklearn.py:967\u001b[0m, in \u001b[0;36mLGBMClassifier.fit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    965\u001b[0m             valid_sets[i] \u001b[38;5;241m=\u001b[39m (valid_x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_le\u001b[38;5;241m.\u001b[39mtransform(valid_y))\n\u001b[0;32m--> 967\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_class_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_class_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/lightgbm/sklearn.py:748\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    745\u001b[0m evals_result \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    746\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[0;32m--> 748\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evals_result:\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evals_result \u001b[38;5;241m=\u001b[39m evals_result\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/lightgbm/engine.py:275\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    273\u001b[0m         booster\u001b[38;5;241m.\u001b[39mset_train_data_name(train_data_name)\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m valid_set, name_valid_set \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(reduced_valid_sets, name_valid_sets):\n\u001b[0;32m--> 275\u001b[0m         \u001b[43mbooster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_valid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname_valid_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m     train_set\u001b[38;5;241m.\u001b[39m_reverse_update_params()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/lightgbm/basic.py:2935\u001b[0m, in \u001b[0;36mBooster.add_valid\u001b[0;34m(self, data, name)\u001b[0m\n\u001b[1;32m   2930\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39m_predictor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__init_predictor:\n\u001b[1;32m   2931\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdd validation data failed, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2932\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou should use same predictor for these data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2933\u001b[0m _safe_call(_LIB\u001b[38;5;241m.\u001b[39mLGBM_BoosterAddValidData(\n\u001b[1;32m   2934\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m-> 2935\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mhandle))\n\u001b[1;32m   2936\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_sets\u001b[38;5;241m.\u001b[39mappend(data)\n\u001b[1;32m   2937\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname_valid_sets\u001b[38;5;241m.\u001b[39mappend(name)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/lightgbm/basic.py:1784\u001b[0m, in \u001b[0;36mDataset.construct\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1781\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_params(reference_params)\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mused_indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1783\u001b[0m     \u001b[38;5;66;03m# create valid\u001b[39;00m\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1786\u001b[0m \u001b[43m                    \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predictor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1787\u001b[0m \u001b[43m                    \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1789\u001b[0m     \u001b[38;5;66;03m# construct subset\u001b[39;00m\n\u001b[1;32m   1790\u001b[0m     used_indices \u001b[38;5;241m=\u001b[39m list_to_1d_numpy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mused_indices, np\u001b[38;5;241m.\u001b[39mint32, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mused_indices\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/lightgbm/basic.py:1474\u001b[0m, in \u001b[0;36mDataset._lazy_init\u001b[0;34m(self, data, label, reference, weight, group, init_score, predictor, silent, feature_name, categorical_feature, params)\u001b[0m\n\u001b[1;32m   1472\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpandas_categorical \u001b[38;5;241m=\u001b[39m reference\u001b[38;5;241m.\u001b[39mpandas_categorical\n\u001b[1;32m   1473\u001b[0m     categorical_feature \u001b[38;5;241m=\u001b[39m reference\u001b[38;5;241m.\u001b[39mcategorical_feature\n\u001b[0;32m-> 1474\u001b[0m data, feature_name, categorical_feature, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpandas_categorical \u001b[38;5;241m=\u001b[39m \u001b[43m_data_from_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1475\u001b[0m \u001b[43m                                                                                     \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1476\u001b[0m \u001b[43m                                                                                     \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1477\u001b[0m \u001b[43m                                                                                     \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpandas_categorical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1478\u001b[0m label \u001b[38;5;241m=\u001b[39m _label_from_pandas(label)\n\u001b[1;32m   1480\u001b[0m \u001b[38;5;66;03m# process for args\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/lightgbm/basic.py:566\u001b[0m, in \u001b[0;36m_data_from_pandas\u001b[0;34m(data, feature_name, categorical_feature, pandas_categorical)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd_DataFrame):\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 566\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput data must be 2 dimensional and non empty.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m feature_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m feature_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    568\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Input data must be 2 dimensional and non empty."
     ]
    }
   ],
   "source": [
    "used_feat = joblib.load('sub/feat_select_lgb_feats.pkl')\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import StratifiedGroupKFold  # type: ignore\n",
    "\n",
    "\n",
    "df_importance_dict = dict()\n",
    "df_oof = df_train[['PID','autherID', 'label']].copy()\n",
    "prediction = df_test[['PID','autherID']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#iter_dict = dict({'CHANNEL_A':4000,'CHANNEL_B':10000,'CHANNEL_C':2500})\n",
    "for ycol in ['label']:\n",
    "#for ycol in [ 'CHANNEL_C']:\n",
    "\n",
    "    print(ycol)\n",
    "    #try:\n",
    "    #    used_feat = feature_dict[ycol].copy()\n",
    "    #except:\n",
    "    #    used_feat = list(feature_dict[ycol])[0]\n",
    "    print(len(used_feat))\n",
    "    \n",
    "    \n",
    "    df_oof[f\"{ycol}_prob\"] = 0\n",
    "    prediction[f\"{ycol}_prob\"] = 0\n",
    "\n",
    "    \n",
    "    df_importance_list = []\n",
    "\n",
    "    folds = 10\n",
    "    seeds =[2020]#,2019,2003,2011,2017, 2027]\n",
    "    for seed in seeds:\n",
    "\n",
    "        model2 =  LGBMClassifier(objective = 'binary',boosting_type='gbdt',num_leaves=34,max_depth=15,learning_rate=0.01,n_estimators=3500,subsample=0.8,\n",
    "                               feature_fraction=0.8,reg_alpha=10,reg_lambda= 12,random_state=seed,is_unbalance=True,metric='logloss',n_jobs=24,bagging_freq=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #model2 = XGBClassifier(boosting_type='gbdt',num_leaves=15,max_depth=6,learning_rate=0.01,n_estimators=3500,subsample=0.8,tree_method = 'gpu_hist',\n",
    "        #                   feature_fraction=0.7,random_state=seed,is_unbalance=True,eval_metric='auc')\n",
    "        \"\"\"\n",
    "        ctb_params = dict(iterations=3000,\n",
    "                          learning_rate=0.01,\n",
    "                          depth=8,\n",
    "                          #cat_features = [],\n",
    "                          l2_leaf_reg=30,\n",
    "                          bootstrap_type='Bernoulli',\n",
    "                          subsample=0.8,\n",
    "                          loss_function='Logloss',\n",
    "                          #eval_metric = 'AUC',\n",
    "                          metric_period=100,\n",
    "                          od_type='Iter',\n",
    "                          od_wait=30,\n",
    "                          task_type='GPU',\n",
    "                          allow_writing_files=False,\n",
    "                          )\n",
    "\n",
    "        print(\"Feature Elimination Performing.\")\n",
    "        model2 = CatBoostClassifier(**ctb_params)\n",
    "        \"\"\"\n",
    "        print(\"================{}==============\".format(seed))\n",
    "        #kfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "        kfold = StratifiedGroupKFold(n_splits=folds)\n",
    "        \n",
    "        for fold_id, (trn_idx, val_idx) in enumerate(\n",
    "                kfold.split(df_train[used_feat], df_train[ycol],groups=df_train['autherID'])):\n",
    "        #for fold_id, (trn_idx, val_idx) in enumerate(\n",
    "        #        kfold.split(df_train[used_feat], df_train[ycol],groups = df_train['autherID'])):\n",
    "            X_train = df_train.iloc[trn_idx][used_feat]\n",
    "            Y_train = df_train.iloc[trn_idx][ycol]\n",
    "\n",
    "            X_val = df_train.iloc[val_idx][used_feat]\n",
    "            Y_val = df_train.iloc[val_idx][ycol]\n",
    "            #df_train.iloc[val_idx,'fold_id'] = fold_id\n",
    "\n",
    "            print('\\nFold_{} Training ================================\\n'.format(fold_id + 1))\n",
    "\n",
    "\n",
    "            lgb_model = model2.fit(X_train[used_feat],Y_train,\n",
    "                              eval_set=[(X_val[used_feat], Y_val)],\n",
    "                              #eval_metric = 'auc',\n",
    "                              verbose=100,\n",
    "                              #early_stopping_rounds=200\n",
    "                                  )\n",
    "\n",
    "\n",
    "\n",
    "            pred_val = lgb_model.predict_proba(X_val[used_feat])[:,1]\n",
    "\n",
    "\n",
    "            df_oof.loc[val_idx, f\"{ycol}_prob\"  ] += pred_val\n",
    "\n",
    "\n",
    "            pred_test = lgb_model.predict_proba(df_test[used_feat])[:,1]\n",
    "            prediction[f\"{ycol}_prob\" ] += pred_test\n",
    "\n",
    "            df_importance = pd.DataFrame({\n",
    "                'column': used_feat,\n",
    "                'importance': lgb_model.feature_importances_,\n",
    "            })\n",
    "            df_importance_list.append(df_importance)\n",
    "            #break\n",
    "\n",
    "            del lgb_model, pred_val, pred_test, X_train, Y_train, X_val, Y_val\n",
    "            gc.collect()\n",
    "            #break\n",
    "            \n",
    "       \n",
    "        df_importance_dict[ycol] = df_importance_list.copy()\n",
    "    #break   \n",
    "LABEL_COLUME = ['label']  \n",
    "prediction[[f\"{ycol}_prob\" for ycol in LABEL_COLUME ]] = prediction[[f\"{ycol}_prob\" for ycol in LABEL_COLUME ]]/(folds*len(seeds))\n",
    "\n",
    "df_oof[[f\"{ycol}_prob\" for ycol in LABEL_COLUME ]] = df_oof[[f\"{ycol}_prob\" for ycol in LABEL_COLUME ]] / len(seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3674d49-79cf-4bff-8513-9e47b955e251",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'oof2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_oof\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_feather\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moof2/feat_select_lgb_oof.feather\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m prediction\u001b[38;5;241m.\u001b[39mto_feather(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moof2/feat_select_lgb_prediction.feather\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/core/frame.py:2793\u001b[0m, in \u001b[0;36mDataFrame.to_feather\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m   2768\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2769\u001b[0m \u001b[38;5;124;03mWrite a DataFrame to the binary Feather format.\u001b[39;00m\n\u001b[1;32m   2770\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2789\u001b[0m \u001b[38;5;124;03msupports custom indices e.g. `to_parquet`.\u001b[39;00m\n\u001b[1;32m   2790\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2791\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeather_format\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_feather\n\u001b[0;32m-> 2793\u001b[0m \u001b[43mto_feather\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/io/feather_format.py:90\u001b[0m, in \u001b[0;36mto_feather\u001b[0;34m(df, path, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39minferred_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m valid_types:\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeather must have string column names\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     92\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m     93\u001b[0m     feather\u001b[38;5;241m.\u001b[39mwrite_feather(df, handles\u001b[38;5;241m.\u001b[39mhandle, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/io/common.py:734\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[0;32m--> 734\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    738\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/io/common.py:597\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    595\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'oof2'"
     ]
    }
   ],
   "source": [
    "df_oof.to_feather('oof2/feat_select_lgb_oof.feather')\n",
    "prediction.to_feather('oof2/feat_select_lgb_prediction.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedca030-d0e9-44dd-985c-157fc71020ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_feat = joblib.load('sub/feat_select_xgb_feats.pkl')\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import StratifiedGroupKFold  # type: ignore\n",
    "\n",
    "\n",
    "df_importance_dict = dict()\n",
    "df_oof = df_train[['PID','autherID', 'label']].copy()\n",
    "prediction = df_test[['PID','autherID']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#iter_dict = dict({'CHANNEL_A':4000,'CHANNEL_B':10000,'CHANNEL_C':2500})\n",
    "for ycol in ['label']:\n",
    "#for ycol in [ 'CHANNEL_C']:\n",
    "\n",
    "    print(ycol)\n",
    "    #try:\n",
    "    #    used_feat = feature_dict[ycol].copy()\n",
    "    #except:\n",
    "    #    used_feat = list(feature_dict[ycol])[0]\n",
    "    print(len(used_feat))\n",
    "    \n",
    "    \n",
    "    df_oof[f\"{ycol}_prob\"] = 0\n",
    "    prediction[f\"{ycol}_prob\"] = 0\n",
    "\n",
    "    \n",
    "    df_importance_list = []\n",
    "\n",
    "    folds = 10\n",
    "    seeds =[2020]#,2019,2003,2011,2017, 2027]\n",
    "    for seed in seeds:\n",
    "\n",
    "        #model2 =  LGBMClassifier(objective = 'binary',boosting_type='gbdt',num_leaves=34,max_depth=15,learning_rate=0.01,n_estimators=3500,subsample=0.8,\n",
    "        #                       feature_fraction=0.8,reg_alpha=10,reg_lambda= 12,random_state=seed,is_unbalance=True,metric='logloss',n_jobs=24,bagging_freq=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        model2 = XGBClassifier(boosting_type='gbdt',num_leaves=15,max_depth=6,learning_rate=0.01,n_estimators=3500,subsample=0.8,tree_method = 'gpu_hist',\n",
    "                           feature_fraction=0.7,random_state=seed,is_unbalance=True,eval_metric='auc')\n",
    "        \"\"\"\n",
    "        ctb_params = dict(iterations=3000,\n",
    "                          learning_rate=0.01,\n",
    "                          depth=8,\n",
    "                          #cat_features = [],\n",
    "                          l2_leaf_reg=30,\n",
    "                          bootstrap_type='Bernoulli',\n",
    "                          subsample=0.8,\n",
    "                          loss_function='Logloss',\n",
    "                          #eval_metric = 'AUC',\n",
    "                          metric_period=100,\n",
    "                          od_type='Iter',\n",
    "                          od_wait=30,\n",
    "                          task_type='GPU',\n",
    "                          allow_writing_files=False,\n",
    "                          )\n",
    "\n",
    "        print(\"Feature Elimination Performing.\")\n",
    "        model2 = CatBoostClassifier(**ctb_params)\n",
    "        \"\"\"\n",
    "        print(\"================{}==============\".format(seed))\n",
    "        #kfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "        kfold = StratifiedGroupKFold(n_splits=folds)\n",
    "        \n",
    "        for fold_id, (trn_idx, val_idx) in enumerate(\n",
    "                kfold.split(df_train[used_feat], df_train[ycol],groups=df_train['autherID'])):\n",
    "        #for fold_id, (trn_idx, val_idx) in enumerate(\n",
    "        #        kfold.split(df_train[used_feat], df_train[ycol],groups = df_train['autherID'])):\n",
    "            X_train = df_train.iloc[trn_idx][used_feat]\n",
    "            Y_train = df_train.iloc[trn_idx][ycol]\n",
    "\n",
    "            X_val = df_train.iloc[val_idx][used_feat]\n",
    "            Y_val = df_train.iloc[val_idx][ycol]\n",
    "            #df_train.iloc[val_idx,'fold_id'] = fold_id\n",
    "\n",
    "            print('\\nFold_{} Training ================================\\n'.format(fold_id + 1))\n",
    "\n",
    "\n",
    "            lgb_model = model2.fit(X_train[used_feat],Y_train,\n",
    "                              eval_set=[(X_val[used_feat], Y_val)],\n",
    "                              #eval_metric = 'auc',\n",
    "                              verbose=100,\n",
    "                              #early_stopping_rounds=200\n",
    "                                  )\n",
    "\n",
    "\n",
    "\n",
    "            pred_val = lgb_model.predict_proba(X_val[used_feat])[:,1]\n",
    "\n",
    "\n",
    "            df_oof.loc[val_idx, f\"{ycol}_prob\"  ] += pred_val\n",
    "\n",
    "\n",
    "            pred_test = lgb_model.predict_proba(df_test[used_feat])[:,1]\n",
    "            prediction[f\"{ycol}_prob\" ] += pred_test\n",
    "\n",
    "            df_importance = pd.DataFrame({\n",
    "                'column': used_feat,\n",
    "                'importance': lgb_model.feature_importances_,\n",
    "            })\n",
    "            df_importance_list.append(df_importance)\n",
    "            #break\n",
    "\n",
    "            del lgb_model, pred_val, pred_test, X_train, Y_train, X_val, Y_val\n",
    "            gc.collect()\n",
    "            #break\n",
    "            \n",
    "       \n",
    "        df_importance_dict[ycol] = df_importance_list.copy()\n",
    "    #break   \n",
    "LABEL_COLUME = ['label']  \n",
    "prediction[[f\"{ycol}_prob\" for ycol in LABEL_COLUME ]] = prediction[[f\"{ycol}_prob\" for ycol in LABEL_COLUME ]]/(folds*len(seeds))\n",
    "\n",
    "df_oof[[f\"{ycol}_prob\" for ycol in LABEL_COLUME ]] = df_oof[[f\"{ycol}_prob\" for ycol in LABEL_COLUME ]] / len(seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba26a86-c40a-4d3c-8973-38180dfa04cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oof.to_feather('oof2/feat_select_xgb_oof.feather')\n",
    "prediction.to_feather('oof2/feat_select_xgb_prediction.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4940bd59-e43e-4888-a01e-590f93f018d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_feat = joblib.load('sub/feat_select_cat_feats.pkl')\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import StratifiedGroupKFold  # type: ignore\n",
    "\n",
    "\n",
    "df_importance_dict = dict()\n",
    "df_oof = df_train[['PID','autherID', 'label']].copy()\n",
    "prediction = df_test[['PID','autherID']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#iter_dict = dict({'CHANNEL_A':4000,'CHANNEL_B':10000,'CHANNEL_C':2500})\n",
    "for ycol in ['label']:\n",
    "#for ycol in [ 'CHANNEL_C']:\n",
    "\n",
    "    print(ycol)\n",
    "    #try:\n",
    "    #    used_feat = feature_dict[ycol].copy()\n",
    "    #except:\n",
    "    #    used_feat = list(feature_dict[ycol])[0]\n",
    "    print(len(used_feat))\n",
    "    \n",
    "    \n",
    "    df_oof[f\"{ycol}_prob\"] = 0\n",
    "    prediction[f\"{ycol}_prob\"] = 0\n",
    "\n",
    "    \n",
    "    df_importance_list = []\n",
    "\n",
    "    folds = 10\n",
    "    seeds =[2020]#,2019,2003,2011,2017, 2027]\n",
    "    for seed in seeds:\n",
    "\n",
    "        #model2 =  LGBMClassifier(objective = 'binary',boosting_type='gbdt',num_leaves=34,max_depth=15,learning_rate=0.01,n_estimators=3500,subsample=0.8,\n",
    "        #                       feature_fraction=0.8,reg_alpha=10,reg_lambda= 12,random_state=seed,is_unbalance=True,metric='logloss',n_jobs=24,bagging_freq=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #model2 = XGBClassifier(boosting_type='gbdt',num_leaves=15,max_depth=6,learning_rate=0.01,n_estimators=3500,subsample=0.8,tree_method = 'gpu_hist',\n",
    "        #                   feature_fraction=0.7,random_state=seed,is_unbalance=True,eval_metric='auc')\n",
    "        \n",
    "        ctb_params = dict(iterations=3000,\n",
    "                          learning_rate=0.01,\n",
    "                          depth=8,\n",
    "                          #cat_features = [],\n",
    "                          l2_leaf_reg=30,\n",
    "                          bootstrap_type='Bernoulli',\n",
    "                          subsample=0.8,\n",
    "                          loss_function='Logloss',\n",
    "                          #eval_metric = 'AUC',\n",
    "                          metric_period=100,\n",
    "                          od_type='Iter',\n",
    "                          od_wait=30,\n",
    "                          task_type='GPU',\n",
    "                          allow_writing_files=False,\n",
    "                          )\n",
    "\n",
    "        print(\"Feature Elimination Performing.\")\n",
    "        model2 = CatBoostClassifier(**ctb_params)\n",
    "        \n",
    "        print(\"================{}==============\".format(seed))\n",
    "        #kfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "        kfold = StratifiedGroupKFold(n_splits=folds)\n",
    "        \n",
    "        for fold_id, (trn_idx, val_idx) in enumerate(\n",
    "                kfold.split(df_train[used_feat], df_train[ycol],groups=df_train['autherID'])):\n",
    "        #for fold_id, (trn_idx, val_idx) in enumerate(\n",
    "        #        kfold.split(df_train[used_feat], df_train[ycol],groups = df_train['autherID'])):\n",
    "            X_train = df_train.iloc[trn_idx][used_feat]\n",
    "            Y_train = df_train.iloc[trn_idx][ycol]\n",
    "\n",
    "            X_val = df_train.iloc[val_idx][used_feat]\n",
    "            Y_val = df_train.iloc[val_idx][ycol]\n",
    "            #df_train.iloc[val_idx,'fold_id'] = fold_id\n",
    "\n",
    "            print('\\nFold_{} Training ================================\\n'.format(fold_id + 1))\n",
    "\n",
    "\n",
    "            lgb_model = model2.fit(X_train[used_feat],Y_train,\n",
    "                              eval_set=[(X_val[used_feat], Y_val)],\n",
    "                              #eval_metric = 'auc',\n",
    "                              verbose=100,\n",
    "                              #early_stopping_rounds=200\n",
    "                                  )\n",
    "\n",
    "\n",
    "\n",
    "            pred_val = lgb_model.predict_proba(X_val[used_feat])[:,1]\n",
    "\n",
    "\n",
    "            df_oof.loc[val_idx, f\"{ycol}_prob\"  ] += pred_val\n",
    "\n",
    "\n",
    "            pred_test = lgb_model.predict_proba(df_test[used_feat])[:,1]\n",
    "            prediction[f\"{ycol}_prob\" ] += pred_test\n",
    "\n",
    "            df_importance = pd.DataFrame({\n",
    "                'column': used_feat,\n",
    "                'importance': lgb_model.feature_importances_,\n",
    "            })\n",
    "            df_importance_list.append(df_importance)\n",
    "            #break\n",
    "\n",
    "            del lgb_model, pred_val, pred_test, X_train, Y_train, X_val, Y_val\n",
    "            gc.collect()\n",
    "            #break\n",
    "            \n",
    "       \n",
    "        df_importance_dict[ycol] = df_importance_list.copy()\n",
    "    #break   \n",
    "LABEL_COLUME = ['label']  \n",
    "prediction[[f\"{ycol}_prob\" for ycol in LABEL_COLUME ]] = prediction[[f\"{ycol}_prob\" for ycol in LABEL_COLUME ]]/(folds*len(seeds))\n",
    "\n",
    "df_oof[[f\"{ycol}_prob\" for ycol in LABEL_COLUME ]] = df_oof[[f\"{ycol}_prob\" for ycol in LABEL_COLUME ]] / len(seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eccea8-d940-4b48-8f82-32d6479aa07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oof.to_feather('oof2/feat_select_cat_oof.feather')\n",
    "prediction.to_feather('oof2/feat_select_cat_prediction.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8e1616-7efe-44a6-8022-3b35795ec527",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
